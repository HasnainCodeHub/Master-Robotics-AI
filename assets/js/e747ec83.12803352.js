"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[7051],{3583(e,i,n){n.r(i),n.d(i,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>t,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"glossary","title":"Glossary","description":"This glossary defines key technical terms used throughout the textbook. Terms are organized alphabetically for easy reference.","source":"@site/docs/glossary.md","sourceDirName":".","slug":"/glossary","permalink":"/Master-Robotics-AI/textbook/glossary","draft":false,"unlisted":false,"editUrl":"https://github.com/HasnainCodeHub/Master-Robotics-AI/tree/main/docs/docs/glossary.md","tags":[],"version":"current","sidebarPosition":100,"frontMatter":{"id":"glossary","title":"Glossary","sidebar_label":"Glossary","sidebar_position":100},"sidebar":"textbookSidebar","previous":{"title":"D. Simulation Installation","permalink":"/Master-Robotics-AI/textbook/appendix/simulation-installation"}}');var a=n(4848),s=n(8453);const t={id:"glossary",title:"Glossary",sidebar_label:"Glossary",sidebar_position:100},r="Glossary of Physical AI & Robotics Terms",l={},d=[{value:"A",id:"a",level:2},{value:"Action (ROS 2)",id:"action-ros-2",level:3},{value:"Actuator",id:"actuator",level:3},{value:"AMCL (Adaptive Monte Carlo Localization)",id:"amcl-adaptive-monte-carlo-localization",level:3},{value:"B",id:"b",level:2},{value:"Base Link",id:"base-link",level:3},{value:"C",id:"c",level:2},{value:"Collision Avoidance",id:"collision-avoidance",level:3},{value:"Costmap",id:"costmap",level:3},{value:"Control Loop",id:"control-loop",level:3},{value:"D",id:"d",level:2},{value:"Depth Camera",id:"depth-camera",level:3},{value:"Digital Twin",id:"digital-twin",level:3},{value:"Domain Randomization",id:"domain-randomization",level:3},{value:"DOF (Degrees of Freedom)",id:"dof-degrees-of-freedom",level:3},{value:"E",id:"e",level:2},{value:"Embodied Intelligence",id:"embodied-intelligence",level:3},{value:"End Effector",id:"end-effector",level:3},{value:"F",id:"f",level:2},{value:"Forward Kinematics",id:"forward-kinematics",level:3},{value:"Frame (Coordinate)",id:"frame-coordinate",level:3},{value:"G",id:"g",level:2},{value:"Gazebo",id:"gazebo",level:3},{value:"Grounding (VLA)",id:"grounding-vla",level:3},{value:"Gripper",id:"gripper",level:3},{value:"H",id:"h",level:2},{value:"Hallucination",id:"hallucination",level:3},{value:"HITL (Human-in-the-Loop)",id:"hitl-human-in-the-loop",level:3},{value:"I",id:"i",level:2},{value:"IMU (Inertial Measurement Unit)",id:"imu-inertial-measurement-unit",level:3},{value:"Inverse Kinematics (IK)",id:"inverse-kinematics-ik",level:3},{value:"Isaac ROS",id:"isaac-ros",level:3},{value:"Isaac Sim",id:"isaac-sim",level:3},{value:"J",id:"j",level:2},{value:"Joint",id:"joint",level:3},{value:"Joint State",id:"joint-state",level:3},{value:"L",id:"l",level:2},{value:"Latency",id:"latency",level:3},{value:"LiDAR (Light Detection and Ranging)",id:"lidar-light-detection-and-ranging",level:3},{value:"Link",id:"link",level:3},{value:"LLM (Large Language Model)",id:"llm-large-language-model",level:3},{value:"M",id:"m",level:2},{value:"MoveIt 2",id:"moveit-2",level:3},{value:"N",id:"n",level:2},{value:"Nav2",id:"nav2",level:3},{value:"NITROS",id:"nitros",level:3},{value:"Node (ROS 2)",id:"node-ros-2",level:3},{value:"O",id:"o",level:2},{value:"Odometry",id:"odometry",level:3},{value:"OMPL (Open Motion Planning Library)",id:"ompl-open-motion-planning-library",level:3},{value:"P",id:"p",level:2},{value:"Perception-Action Loop",id:"perception-action-loop",level:3},{value:"PhysX",id:"physx",level:3},{value:"Point Cloud",id:"point-cloud",level:3},{value:"Pose",id:"pose",level:3},{value:"Q",id:"q",level:2},{value:"QoS (Quality of Service)",id:"qos-quality-of-service",level:3},{value:"Quaternion",id:"quaternion",level:3},{value:"R",id:"r",level:2},{value:"Reality Gap",id:"reality-gap",level:3},{value:"ROS 2 (Robot Operating System 2)",id:"ros-2-robot-operating-system-2",level:3},{value:"RTF (Real-Time Factor)",id:"rtf-real-time-factor",level:3},{value:"S",id:"s",level:2},{value:"Service (ROS 2)",id:"service-ros-2",level:3},{value:"Sim-to-Real Transfer",id:"sim-to-real-transfer",level:3},{value:"SLAM (Simultaneous Localization and Mapping)",id:"slam-simultaneous-localization-and-mapping",level:3},{value:"T",id:"t",level:2},{value:"tf2",id:"tf2",level:3},{value:"Topic (ROS 2)",id:"topic-ros-2",level:3},{value:"Transform",id:"transform",level:3},{value:"U",id:"u",level:2},{value:"URDF (Unified Robot Description Format)",id:"urdf-unified-robot-description-format",level:3},{value:"USD (Universal Scene Description)",id:"usd-universal-scene-description",level:3},{value:"V",id:"v",level:2},{value:"VLA (Vision-Language-Action)",id:"vla-vision-language-action",level:3},{value:"VLM (Vision-Language Model)",id:"vlm-vision-language-model",level:3},{value:"VSLAM (Visual SLAM)",id:"vslam-visual-slam",level:3},{value:"W",id:"w",level:2},{value:"Whisper",id:"whisper",level:3},{value:"Workspace",id:"workspace",level:3},{value:"X-Z",id:"x-z",level:2},{value:"Xacro",id:"xacro",level:3},{value:"Zero-Copy",id:"zero-copy",level:3},{value:"Related Resources",id:"related-resources",level:2}];function c(e){const i={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(i.header,{children:(0,a.jsx)(i.h1,{id:"glossary-of-physical-ai--robotics-terms",children:"Glossary of Physical AI & Robotics Terms"})}),"\n",(0,a.jsx)(i.p,{children:"This glossary defines key technical terms used throughout the textbook. Terms are organized alphabetically for easy reference."}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"a",children:"A"}),"\n",(0,a.jsx)(i.h3,{id:"action-ros-2",children:"Action (ROS 2)"}),"\n",(0,a.jsx)(i.p,{children:"A ROS 2 communication pattern for long-running tasks with feedback. Actions allow cancellation and progress monitoring, unlike services which are blocking. Example: navigation to a goal provides distance feedback during execution."}),"\n",(0,a.jsx)(i.h3,{id:"actuator",children:"Actuator"}),"\n",(0,a.jsx)(i.p,{children:"A component that converts control signals into physical motion or force. Common types include electric motors, hydraulic cylinders, and pneumatic actuators. Key parameters: torque, speed, position accuracy."}),"\n",(0,a.jsx)(i.h3,{id:"amcl-adaptive-monte-carlo-localization",children:"AMCL (Adaptive Monte Carlo Localization)"}),"\n",(0,a.jsx)(i.p,{children:"A probabilistic localization algorithm that estimates robot pose using a particle filter. Commonly used with 2D LiDAR for indoor navigation."}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"b",children:"B"}),"\n",(0,a.jsx)(i.h3,{id:"base-link",children:"Base Link"}),"\n",(0,a.jsx)(i.p,{children:"The root link in a robot's kinematic chain, typically attached to the main body. All other links are defined relative to this frame."}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"c",children:"C"}),"\n",(0,a.jsx)(i.h3,{id:"collision-avoidance",children:"Collision Avoidance"}),"\n",(0,a.jsx)(i.p,{children:"Techniques to prevent a robot from colliding with obstacles or itself. Includes reactive methods (stopping when too close) and predictive methods (planning collision-free paths)."}),"\n",(0,a.jsx)(i.h3,{id:"costmap",children:"Costmap"}),"\n",(0,a.jsx)(i.p,{children:"A grid-based representation of traversability used for navigation. Cells contain cost values indicating obstacles, inflation zones, or free space."}),"\n",(0,a.jsx)(i.h3,{id:"control-loop",children:"Control Loop"}),"\n",(0,a.jsx)(i.p,{children:"The continuous cycle of reading sensors, computing control outputs, and commanding actuators. Loop frequency determines response speed and stability."}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"d",children:"D"}),"\n",(0,a.jsx)(i.h3,{id:"depth-camera",children:"Depth Camera"}),"\n",(0,a.jsx)(i.p,{children:"A sensor that captures depth (distance) information per pixel. Technologies include structured light (e.g., RealSense), time-of-flight, and stereo vision. Output: depth images or point clouds."}),"\n",(0,a.jsx)(i.h3,{id:"digital-twin",children:"Digital Twin"}),"\n",(0,a.jsx)(i.p,{children:"A virtual replica of a physical system that mirrors its state and behavior. Used for testing, monitoring, and simulation."}),"\n",(0,a.jsx)(i.h3,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,a.jsx)(i.p,{children:"A sim-to-real technique that varies simulation parameters (textures, lighting, physics) during training to improve transfer to reality."}),"\n",(0,a.jsx)(i.h3,{id:"dof-degrees-of-freedom",children:"DOF (Degrees of Freedom)"}),"\n",(0,a.jsx)(i.p,{children:"The number of independent parameters needed to specify a robot's configuration. A 6-DOF arm can reach positions and orientations in 3D space."}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"e",children:"E"}),"\n",(0,a.jsx)(i.h3,{id:"embodied-intelligence",children:"Embodied Intelligence"}),"\n",(0,a.jsx)(i.p,{children:"AI systems that exist within physical bodies and must interact with the physical world through sensors and actuators, as opposed to disembodied AI that operates purely in software."}),"\n",(0,a.jsx)(i.h3,{id:"end-effector",children:"End Effector"}),"\n",(0,a.jsx)(i.p,{children:"The device at the end of a robot arm that interacts with the environment. Examples: grippers, suction cups, welding tools."}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"f",children:"F"}),"\n",(0,a.jsx)(i.h3,{id:"forward-kinematics",children:"Forward Kinematics"}),"\n",(0,a.jsx)(i.p,{children:"Computing the end effector pose given joint angles. Unambiguous: each joint configuration produces exactly one pose."}),"\n",(0,a.jsx)(i.h3,{id:"frame-coordinate",children:"Frame (Coordinate)"}),"\n",(0,a.jsx)(i.p,{children:"A coordinate system attached to a point on the robot or in the environment. Transformations between frames enable spatial reasoning."}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"g",children:"G"}),"\n",(0,a.jsx)(i.h3,{id:"gazebo",children:"Gazebo"}),"\n",(0,a.jsx)(i.p,{children:"An open-source robotics simulator supporting physics, sensors, and ROS integration. Part of the Open Robotics ecosystem."}),"\n",(0,a.jsx)(i.h3,{id:"grounding-vla",children:"Grounding (VLA)"}),"\n",(0,a.jsx)(i.p,{children:'The process of mapping symbolic references (e.g., "red mug") to physical quantities (e.g., 3D position). Connects language to robot perception.'}),"\n",(0,a.jsx)(i.h3,{id:"gripper",children:"Gripper"}),"\n",(0,a.jsx)(i.p,{children:"An end effector designed to grasp objects. Types include parallel jaw, three-finger, and suction grippers."}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"h",children:"H"}),"\n",(0,a.jsx)(i.h3,{id:"hallucination",children:"Hallucination"}),"\n",(0,a.jsx)(i.p,{children:"When an AI system generates plausible but factually incorrect information. In robotics, VLMs may hallucinate objects that don't exist."}),"\n",(0,a.jsx)(i.h3,{id:"hitl-human-in-the-loop",children:"HITL (Human-in-the-Loop)"}),"\n",(0,a.jsx)(i.p,{children:"System design where human operators can intervene, provide guidance, or override automated decisions. Essential for safety-critical operations."}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"i",children:"I"}),"\n",(0,a.jsx)(i.h3,{id:"imu-inertial-measurement-unit",children:"IMU (Inertial Measurement Unit)"}),"\n",(0,a.jsx)(i.p,{children:"A sensor combining accelerometers and gyroscopes to measure linear acceleration and angular velocity. Used for odometry and state estimation."}),"\n",(0,a.jsx)(i.h3,{id:"inverse-kinematics-ik",children:"Inverse Kinematics (IK)"}),"\n",(0,a.jsx)(i.p,{children:"Computing joint angles that achieve a desired end effector pose. May have multiple solutions or no solution."}),"\n",(0,a.jsx)(i.h3,{id:"isaac-ros",children:"Isaac ROS"}),"\n",(0,a.jsx)(i.p,{children:"NVIDIA's GPU-accelerated ROS packages for perception and navigation. Uses NITROS for zero-copy GPU pipeline communication."}),"\n",(0,a.jsx)(i.h3,{id:"isaac-sim",children:"Isaac Sim"}),"\n",(0,a.jsx)(i.p,{children:"NVIDIA's GPU-based robotics simulator built on Omniverse. Features RTX rendering, PhysX physics, and synthetic data generation."}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"j",children:"J"}),"\n",(0,a.jsx)(i.h3,{id:"joint",children:"Joint"}),"\n",(0,a.jsx)(i.p,{children:"A connection between two links that allows relative motion. Types: revolute (rotation), prismatic (linear), fixed (no motion)."}),"\n",(0,a.jsx)(i.h3,{id:"joint-state",children:"Joint State"}),"\n",(0,a.jsxs)(i.p,{children:["The current position, velocity, and effort of each robot joint. Published on ",(0,a.jsx)(i.code,{children:"/joint_states"})," in ROS 2."]}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"l",children:"L"}),"\n",(0,a.jsx)(i.h3,{id:"latency",children:"Latency"}),"\n",(0,a.jsx)(i.p,{children:"Time delay between input and output. In robotics, includes sensor capture, processing, communication, and actuation delays. Critical for real-time control."}),"\n",(0,a.jsx)(i.h3,{id:"lidar-light-detection-and-ranging",children:"LiDAR (Light Detection and Ranging)"}),"\n",(0,a.jsx)(i.p,{children:"A sensor that measures distances using laser pulses. Produces 2D scans or 3D point clouds. Key parameters: range, resolution, scan rate."}),"\n",(0,a.jsx)(i.h3,{id:"link",children:"Link"}),"\n",(0,a.jsx)(i.p,{children:"A rigid body segment in a robot's kinematic chain. Connected by joints."}),"\n",(0,a.jsx)(i.h3,{id:"llm-large-language-model",children:"LLM (Large Language Model)"}),"\n",(0,a.jsx)(i.p,{children:"A neural network trained on text data that can understand and generate natural language. Used in VLA for task planning."}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"m",children:"M"}),"\n",(0,a.jsx)(i.h3,{id:"moveit-2",children:"MoveIt 2"}),"\n",(0,a.jsx)(i.p,{children:"A motion planning framework for ROS 2. Provides inverse kinematics, collision checking, and path planning for manipulation."}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"n",children:"N"}),"\n",(0,a.jsx)(i.h3,{id:"nav2",children:"Nav2"}),"\n",(0,a.jsx)(i.p,{children:"The ROS 2 navigation stack. Provides autonomous navigation including localization, path planning, and obstacle avoidance."}),"\n",(0,a.jsx)(i.h3,{id:"nitros",children:"NITROS"}),"\n",(0,a.jsx)(i.p,{children:"NVIDIA Isaac Transport for ROS. A framework for GPU-accelerated, zero-copy communication between ROS nodes."}),"\n",(0,a.jsx)(i.h3,{id:"node-ros-2",children:"Node (ROS 2)"}),"\n",(0,a.jsx)(i.p,{children:"An executable that performs computation. Nodes communicate via topics, services, and actions. Each node should have a single responsibility."}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"o",children:"O"}),"\n",(0,a.jsx)(i.h3,{id:"odometry",children:"Odometry"}),"\n",(0,a.jsx)(i.p,{children:"Estimation of position change over time using wheel encoders, IMU, or visual features. Subject to drift without absolute references."}),"\n",(0,a.jsx)(i.h3,{id:"ompl-open-motion-planning-library",children:"OMPL (Open Motion Planning Library)"}),"\n",(0,a.jsx)(i.p,{children:"A library of sampling-based motion planning algorithms (RRT, PRM, etc.) used by MoveIt for path planning."}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"p",children:"P"}),"\n",(0,a.jsx)(i.h3,{id:"perception-action-loop",children:"Perception-Action Loop"}),"\n",(0,a.jsx)(i.p,{children:"The continuous cycle of sensing the environment, processing information, deciding on actions, and executing them. Fundamental to all robotic systems."}),"\n",(0,a.jsx)(i.h3,{id:"physx",children:"PhysX"}),"\n",(0,a.jsx)(i.p,{children:"NVIDIA's physics simulation engine. Used in Isaac Sim for rigid body dynamics, articulations, and contact physics."}),"\n",(0,a.jsx)(i.h3,{id:"point-cloud",children:"Point Cloud"}),"\n",(0,a.jsx)(i.p,{children:"A set of 3D points representing a surface or scene. Generated by LiDAR or depth cameras."}),"\n",(0,a.jsx)(i.h3,{id:"pose",children:"Pose"}),"\n",(0,a.jsx)(i.p,{children:"A combination of position (x, y, z) and orientation (quaternion or Euler angles). Represents an object's location and facing direction in space."}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"q",children:"Q"}),"\n",(0,a.jsx)(i.h3,{id:"qos-quality-of-service",children:"QoS (Quality of Service)"}),"\n",(0,a.jsx)(i.p,{children:"ROS 2 communication settings controlling reliability, durability, and history. Configured per topic to match sensor/control requirements."}),"\n",(0,a.jsx)(i.h3,{id:"quaternion",children:"Quaternion"}),"\n",(0,a.jsx)(i.p,{children:"A four-component representation of 3D rotation (x, y, z, w). Avoids gimbal lock and is numerically stable."}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"r",children:"R"}),"\n",(0,a.jsx)(i.h3,{id:"reality-gap",children:"Reality Gap"}),"\n",(0,a.jsx)(i.p,{children:"The difference between simulated and real-world behavior. Caused by simplified physics, sensor models, and unmodeled dynamics. A key challenge in sim-to-real transfer."}),"\n",(0,a.jsx)(i.h3,{id:"ros-2-robot-operating-system-2",children:"ROS 2 (Robot Operating System 2)"}),"\n",(0,a.jsx)(i.p,{children:"A middleware framework for robotics providing communication, tools, and libraries. The standard for robotics software development."}),"\n",(0,a.jsx)(i.h3,{id:"rtf-real-time-factor",children:"RTF (Real-Time Factor)"}),"\n",(0,a.jsx)(i.p,{children:"The ratio of simulated time to wall-clock time. RTF=1.0 means simulation runs at real speed; RTF < 1.0 means slower."}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"s",children:"S"}),"\n",(0,a.jsx)(i.h3,{id:"service-ros-2",children:"Service (ROS 2)"}),"\n",(0,a.jsx)(i.p,{children:"A ROS 2 communication pattern for synchronous request-response interactions. Blocking: the caller waits for a response."}),"\n",(0,a.jsx)(i.h3,{id:"sim-to-real-transfer",children:"Sim-to-Real Transfer"}),"\n",(0,a.jsx)(i.p,{children:"Deploying policies or models trained in simulation to physical robots. Requires addressing the reality gap."}),"\n",(0,a.jsx)(i.h3,{id:"slam-simultaneous-localization-and-mapping",children:"SLAM (Simultaneous Localization and Mapping)"}),"\n",(0,a.jsx)(i.p,{children:"Algorithms that build a map while simultaneously localizing within it. Essential for autonomous navigation in unknown environments."}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"t",children:"T"}),"\n",(0,a.jsx)(i.h3,{id:"tf2",children:"tf2"}),"\n",(0,a.jsx)(i.p,{children:"ROS 2's transform library. Manages coordinate frame relationships over time, enabling spatial reasoning across the robot."}),"\n",(0,a.jsx)(i.h3,{id:"topic-ros-2",children:"Topic (ROS 2)"}),"\n",(0,a.jsx)(i.p,{children:"A ROS 2 communication channel for asynchronous publish-subscribe messaging. Named streams of typed messages."}),"\n",(0,a.jsx)(i.h3,{id:"transform",children:"Transform"}),"\n",(0,a.jsx)(i.p,{children:"The mathematical relationship (translation and rotation) between two coordinate frames."}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"u",children:"U"}),"\n",(0,a.jsx)(i.h3,{id:"urdf-unified-robot-description-format",children:"URDF (Unified Robot Description Format)"}),"\n",(0,a.jsx)(i.p,{children:"An XML format describing robot structure: links, joints, visuals, collisions, and inertias. Used for simulation and motion planning."}),"\n",(0,a.jsx)(i.h3,{id:"usd-universal-scene-description",children:"USD (Universal Scene Description)"}),"\n",(0,a.jsx)(i.p,{children:"Pixar's scene description format adopted by NVIDIA Omniverse. Supports composition, layering, and real-time collaboration."}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"v",children:"V"}),"\n",(0,a.jsx)(i.h3,{id:"vla-vision-language-action",children:"VLA (Vision-Language-Action)"}),"\n",(0,a.jsx)(i.p,{children:"Systems that combine vision understanding, language processing, and action execution. Enable natural language control of robots."}),"\n",(0,a.jsx)(i.h3,{id:"vlm-vision-language-model",children:"VLM (Vision-Language Model)"}),"\n",(0,a.jsx)(i.p,{children:"Neural networks that process both images and text, enabling visual question answering and scene understanding."}),"\n",(0,a.jsx)(i.h3,{id:"vslam-visual-slam",children:"VSLAM (Visual SLAM)"}),"\n",(0,a.jsx)(i.p,{children:"SLAM using camera data (monocular, stereo, or RGB-D) rather than LiDAR. Provides rich feature information but is sensitive to lighting."}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"w",children:"W"}),"\n",(0,a.jsx)(i.h3,{id:"whisper",children:"Whisper"}),"\n",(0,a.jsx)(i.p,{children:"OpenAI's automatic speech recognition model. Used in VLA for voice command input."}),"\n",(0,a.jsx)(i.h3,{id:"workspace",children:"Workspace"}),"\n",(0,a.jsx)(i.p,{children:"The set of all positions reachable by a robot's end effector. Defined by kinematic limits and obstacle locations."}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"x-z",children:"X-Z"}),"\n",(0,a.jsx)(i.h3,{id:"xacro",children:"Xacro"}),"\n",(0,a.jsx)(i.p,{children:"An XML macro language extending URDF. Enables reusable, parameterized robot descriptions."}),"\n",(0,a.jsx)(i.h3,{id:"zero-copy",children:"Zero-Copy"}),"\n",(0,a.jsx)(i.p,{children:"Memory optimization where data is shared rather than copied between processes. Used in NITROS for GPU pipeline efficiency."}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"related-resources",children:"Related Resources"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"ROS 2 Documentation"}),": ",(0,a.jsx)(i.a,{href:"https://docs.ros.org/en/humble/",children:"https://docs.ros.org/en/humble/"})]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"MoveIt 2 Documentation"}),": ",(0,a.jsx)(i.a,{href:"https://moveit.picknik.ai/",children:"https://moveit.picknik.ai/"})]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Isaac Sim Documentation"}),": ",(0,a.jsx)(i.a,{href:"https://docs.omniverse.nvidia.com/isaacsim/",children:"https://docs.omniverse.nvidia.com/isaacsim/"})]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Gazebo Documentation"}),": ",(0,a.jsx)(i.a,{href:"https://gazebosim.org/docs",children:"https://gazebosim.org/docs"})]}),"\n"]})]})}function h(e={}){const{wrapper:i}={...(0,s.R)(),...e.components};return i?(0,a.jsx)(i,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453(e,i,n){n.d(i,{R:()=>t,x:()=>r});var o=n(6540);const a={},s=o.createContext(a);function t(e){const i=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function r(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),o.createElement(s.Provider,{value:i},e.children)}}}]);