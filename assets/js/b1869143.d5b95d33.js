"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[9523],{3723(e,n,r){r.r(n),r.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-5/chapter-4-vision-language-models","title":"Chapter 4: Vision-Language Models","description":"Chapter Goal","source":"@site/docs/module-5/chapter-4-vision-language-models.md","sourceDirName":"module-5","slug":"/module-5/chapter-4-vision-language-models","permalink":"/Master-Robotics-AI/textbook/module-5/chapter-4-vision-language-models","draft":false,"unlisted":false,"editUrl":"https://github.com/HasnainCodeHub/Master-Robotics-AI/tree/main/docs/docs/module-5/chapter-4-vision-language-models.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"id":"chapter-4-vision-language-models","title":"Chapter 4: Vision-Language Models","sidebar_label":"4. Vision-Language Models","sidebar_position":5},"sidebar":"textbookSidebar","previous":{"title":"3. Grounding","permalink":"/Master-Robotics-AI/textbook/module-5/chapter-3-grounding"},"next":{"title":"5. Safety Constraints","permalink":"/Master-Robotics-AI/textbook/module-5/chapter-5-safety-constraints"}}');var i=r(4848),s=r(8453);const l={id:"chapter-4-vision-language-models",title:"Chapter 4: Vision-Language Models",sidebar_label:"4. Vision-Language Models",sidebar_position:5},o="Chapter 4: Vision-Language Models for Scene Understanding",a={},c=[{value:"Chapter Goal",id:"chapter-goal",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"VLMs vs Object Detection",id:"comparison",level:2},{value:"Complementary Capabilities",id:"complementary-capabilities",level:3},{value:"When to Use VLM",id:"when-to-use-vlm",level:3},{value:"VLM Architecture Overview",id:"architecture",level:2},{value:"How VLMs Work",id:"how-vlms-work",level:3},{value:"Common VLMs",id:"common-vlms",level:3},{value:"VLM Deployment",id:"deployment",level:2},{value:"Using LLaVA (Local)",id:"using-llava-local",level:3},{value:"Using GPT-4V (API)",id:"using-gpt-4v-api",level:3},{value:"Prompt Engineering for Robotics",id:"prompts",level:2},{value:"Scene Understanding Prompts",id:"scene-understanding-prompts",level:3},{value:"Object Identification Prompts",id:"object-identification-prompts",level:3},{value:"Integration with Grounding",id:"grounding-integration",level:2},{value:"VLM-Enhanced Object Resolution",id:"vlm-enhanced-object-resolution",level:3},{value:"VLM Limitations",id:"limitations",level:2},{value:"Hallucination Examples",id:"hallucination-examples",level:3},{value:"Reliability Evaluation",id:"reliability-evaluation",level:3},{value:"Latency Considerations",id:"latency",level:2},{value:"VLM Latency in VLA Pipeline",id:"vlm-latency-in-vla-pipeline",level:3},{value:"Latency Mitigation",id:"latency-mitigation",level:3},{value:"Summary",id:"summary",level:2},{value:"Safety Callout",id:"safety-callout",level:2},{value:"Self-Assessment Questions",id:"self-assessment-questions",level:2},{value:"What&#39;s Next",id:"whats-next",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-4-vision-language-models-for-scene-understanding",children:"Chapter 4: Vision-Language Models for Scene Understanding"})}),"\n",(0,i.jsx)(n.h2,{id:"chapter-goal",children:"Chapter Goal"}),"\n",(0,i.jsxs)(n.p,{children:["By the end of this chapter, you will be able to ",(0,i.jsx)(n.strong,{children:"integrate vision-language models (VLMs) for scene understanding"}),", enabling the robot to answer questions about its camera view and ground visual references in natural language commands."]}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"ID"}),(0,i.jsx)(n.th,{children:"Objective"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"4.1"}),(0,i.jsx)(n.td,{children:"Explain VLM architecture and capabilities for robotics"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"4.2"}),(0,i.jsx)(n.td,{children:"Deploy a VLM for robot camera image understanding"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"4.3"}),(0,i.jsx)(n.td,{children:"Design prompts that extract robotics-relevant information"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"4.4"}),(0,i.jsx)(n.td,{children:"Implement VLM-based object identification for grounding"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"4.5"}),(0,i.jsx)(n.td,{children:"Evaluate VLM reliability and latency"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"4.6"}),(0,i.jsx)(n.td,{children:"Integrate VLM with the grounding layer"})]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"comparison",children:"VLMs vs Object Detection"}),"\n",(0,i.jsx)(n.h3,{id:"complementary-capabilities",children:"Complementary Capabilities"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Capability"}),(0,i.jsx)(n.th,{children:"Object Detection"}),(0,i.jsx)(n.th,{children:"VLM"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Fixed classes"}),(0,i.jsx)(n.td,{children:"Yes"}),(0,i.jsx)(n.td,{children:"No (open vocabulary)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Spatial relations"}),(0,i.jsx)(n.td,{children:"No"}),(0,i.jsx)(n.td,{children:"Yes"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Object descriptions"}),(0,i.jsx)(n.td,{children:"No"}),(0,i.jsx)(n.td,{children:"Yes"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Speed"}),(0,i.jsx)(n.td,{children:"Fast (10-50ms)"}),(0,i.jsx)(n.td,{children:"Slow (1-10s)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Reliability"}),(0,i.jsx)(n.td,{children:"High (trained)"}),(0,i.jsx)(n.td,{children:"Variable (hallucination)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Arbitrary queries"}),(0,i.jsx)(n.td,{children:"No"}),(0,i.jsx)(n.td,{children:"Yes"})]})]})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Best Practice"}),": Use both. Object detection for reliable, fast detection. VLM for flexible queries and disambiguation."]}),"\n",(0,i.jsx)(n.h3,{id:"when-to-use-vlm",children:"When to Use VLM"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'"Pick up the red mug" \u2192 Object detection (has "mug" class)\r\n"Pick up the thing next to the keyboard" \u2192 VLM (spatial relation)\r\n"Pick up what I\'m pointing at" \u2192 VLM (requires understanding)\r\n"Pick up the broken one" \u2192 VLM (attribute not in detector)\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"architecture",children:"VLM Architecture Overview"}),"\n",(0,i.jsx)(n.h3,{id:"how-vlms-work",children:"How VLMs Work"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Image \u2500\u2500\u25ba Vision Encoder \u2500\u2500\u25ba Image Embeddings\r\n                                    \u2502\r\n                                    \u25bc\r\nText Query \u2500\u2500\u25ba Text Encoder \u2500\u2500\u25ba Fusion \u2500\u2500\u25ba LLM Decoder \u2500\u2500\u25ba Response\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Vision Encoder"}),": Processes image to feature vectors (e.g., CLIP, ViT)"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Fusion"}),": Combines image and text representations"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"LLM Decoder"}),": Generates text response conditioned on image and query"]}),"\n",(0,i.jsx)(n.h3,{id:"common-vlms",children:"Common VLMs"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{children:"Access"}),(0,i.jsx)(n.th,{children:"Latency"}),(0,i.jsx)(n.th,{children:"Quality"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"GPT-4V"}),(0,i.jsx)(n.td,{children:"API"}),(0,i.jsx)(n.td,{children:"2-5s"}),(0,i.jsx)(n.td,{children:"Best"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"LLaVA"}),(0,i.jsx)(n.td,{children:"Local"}),(0,i.jsx)(n.td,{children:"1-3s"}),(0,i.jsx)(n.td,{children:"Good"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"BLIP-2"}),(0,i.jsx)(n.td,{children:"Local"}),(0,i.jsx)(n.td,{children:"1-2s"}),(0,i.jsx)(n.td,{children:"Good"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"MiniGPT-4"}),(0,i.jsx)(n.td,{children:"Local"}),(0,i.jsx)(n.td,{children:"1-3s"}),(0,i.jsx)(n.td,{children:"Moderate"})]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"deployment",children:"VLM Deployment"}),"\n",(0,i.jsx)(n.h3,{id:"using-llava-local",children:"Using LLaVA (Local)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""Local VLM deployment with LLaVA."""\r\n\r\nimport torch\r\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration\r\nfrom PIL import Image\r\nimport numpy as np\r\nimport time\r\n\r\n\r\nclass VLMNode:\r\n    def __init__(self, model_name: str = "llava-hf/llava-1.5-7b-hf"):\r\n        # Load model\r\n        self.processor = AutoProcessor.from_pretrained(model_name)\r\n        self.model = LlavaForConditionalGeneration.from_pretrained(\r\n            model_name,\r\n            torch_dtype=torch.float16,\r\n            device_map="auto"\r\n        )\r\n\r\n    def query(self, image: np.ndarray, question: str) -> tuple[str, float]:\r\n        """\r\n        Query VLM about image.\r\n\r\n        Args:\r\n            image: RGB image as numpy array\r\n            question: Natural language question\r\n\r\n        Returns:\r\n            (answer, latency_ms)\r\n        """\r\n        start = time.time()\r\n\r\n        # Convert to PIL\r\n        pil_image = Image.fromarray(image)\r\n\r\n        # Format prompt\r\n        prompt = f"USER: <image>\\n{question}\\nASSISTANT:"\r\n\r\n        # Process\r\n        inputs = self.processor(\r\n            text=prompt,\r\n            images=pil_image,\r\n            return_tensors="pt"\r\n        ).to(self.model.device)\r\n\r\n        # Generate\r\n        outputs = self.model.generate(\r\n            **inputs,\r\n            max_new_tokens=256,\r\n            do_sample=False  # Deterministic\r\n        )\r\n\r\n        # Decode\r\n        response = self.processor.decode(outputs[0], skip_special_tokens=True)\r\n        answer = response.split("ASSISTANT:")[-1].strip()\r\n\r\n        latency = (time.time() - start) * 1000\r\n        return answer, latency\n'})}),"\n",(0,i.jsx)(n.h3,{id:"using-gpt-4v-api",children:"Using GPT-4V (API)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import base64\r\nfrom openai import OpenAI\r\nimport numpy as np\r\nimport cv2\r\nimport time\r\n\r\n\r\nclass GPT4VNode:\r\n    def __init__(self):\r\n        self.client = OpenAI()\r\n\r\n    def query(self, image: np.ndarray, question: str) -> tuple[str, float]:\r\n        """Query GPT-4V about image."""\r\n        start = time.time()\r\n\r\n        # Encode image to base64\r\n        _, buffer = cv2.imencode(\'.jpg\', cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\r\n        base64_image = base64.b64encode(buffer).decode(\'utf-8\')\r\n\r\n        response = self.client.chat.completions.create(\r\n            model="gpt-4-vision-preview",\r\n            messages=[\r\n                {\r\n                    "role": "user",\r\n                    "content": [\r\n                        {"type": "text", "text": question},\r\n                        {\r\n                            "type": "image_url",\r\n                            "image_url": {\r\n                                "url": f"data:image/jpeg;base64,{base64_image}"\r\n                            }\r\n                        }\r\n                    ]\r\n                }\r\n            ],\r\n            max_tokens=300\r\n        )\r\n\r\n        answer = response.choices[0].message.content\r\n        latency = (time.time() - start) * 1000\r\n\r\n        return answer, latency\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"prompts",children:"Prompt Engineering for Robotics"}),"\n",(0,i.jsx)(n.h3,{id:"scene-understanding-prompts",children:"Scene Understanding Prompts"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'SCENE_PROMPTS = {\r\n    "object_list": """List all objects visible in this image.\r\nFormat your response as a JSON array of objects with fields:\r\n- name: object name\r\n- color: primary color\r\n- location: general location (left, right, center, etc.)\r\nExample: [{"name": "mug", "color": "red", "location": "center"}]""",\r\n\r\n    "object_location": """Where is the {object} in this image?\r\nDescribe its location relative to other objects.\r\nIf not visible, respond with "NOT_FOUND".""",\r\n\r\n    "spatial_relation": """Is there {relation} {object} in this image?\r\nExamples: "a cup on the table", "something next to the keyboard"\r\nRespond with YES or NO, then briefly explain.""",\r\n\r\n    "reference_resolution": """The user said "{reference}".\r\nLooking at this image, which specific object are they referring to?\r\nDescribe it precisely (color, location, distinguishing features).\r\nIf ambiguous or unclear, list all possibilities.""",\r\n}\n'})}),"\n",(0,i.jsx)(n.h3,{id:"object-identification-prompts",children:"Object Identification Prompts"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def create_identification_prompt(reference: str) -> str:\r\n    return f"""You are helping a robot identify objects.\r\n\r\nThe user wants the robot to interact with: "{reference}"\r\n\r\nLooking at this image:\r\n1. Can you identify what the user is referring to?\r\n2. If yes, describe its exact location (which part of the image: top-left, center, etc.)\r\n3. If there are multiple possibilities, list them all\r\n4. If the object is not visible, say "NOT_FOUND"\r\n\r\nResponse format:\r\nFOUND: [yes/no/ambiguous]\r\nLOCATION: [description or N/A]\r\nCANDIDATES: [list if ambiguous, empty otherwise]\r\nCONFIDENCE: [high/medium/low]"""\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"grounding-integration",children:"Integration with Grounding"}),"\n",(0,i.jsx)(n.h3,{id:"vlm-enhanced-object-resolution",children:"VLM-Enhanced Object Resolution"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class VLMEnhancedResolver:\r\n    def __init__(self, vlm, object_detector):\r\n        self.vlm = vlm\r\n        self.detector = object_detector\r\n\r\n    def resolve(\r\n        self,\r\n        reference: str,\r\n        image: np.ndarray\r\n    ) -> Optional[DetectedObject]:\r\n        """\r\n        Resolve reference using VLM + detector.\r\n\r\n        Strategy:\r\n        1. Try detector first (fast, reliable)\r\n        2. If ambiguous/not found, use VLM\r\n        3. Match VLM response to detections\r\n        """\r\n        # Try detector first\r\n        detections = self.detector.detect(image)\r\n        detector_match = self.match_reference(reference, detections)\r\n\r\n        if detector_match and detector_match[\'confidence\'] > 0.8:\r\n            return detector_match[\'object\']\r\n\r\n        # Use VLM for disambiguation\r\n        vlm_response, latency = self.vlm.query(\r\n            image,\r\n            f"Where is {reference} in this image? "\r\n            "Describe its exact position and appearance."\r\n        )\r\n\r\n        # Match VLM description to detections\r\n        for det in detections:\r\n            if self.vlm_matches_detection(vlm_response, det, image):\r\n                return det\r\n\r\n        return None\r\n\r\n    def vlm_matches_detection(\r\n        self,\r\n        vlm_description: str,\r\n        detection: DetectedObject,\r\n        image: np.ndarray\r\n    ) -> bool:\r\n        """Check if VLM description matches detection."""\r\n        # Get detection location description\r\n        img_h, img_w = image.shape[:2]\r\n        det_x = detection.bbox_2d[0] + detection.bbox_2d[2] / 2\r\n        det_y = detection.bbox_2d[1] + detection.bbox_2d[3] / 2\r\n\r\n        location_terms = []\r\n        if det_x < img_w / 3:\r\n            location_terms.append("left")\r\n        elif det_x > 2 * img_w / 3:\r\n            location_terms.append("right")\r\n        else:\r\n            location_terms.append("center")\r\n\r\n        if det_y < img_h / 3:\r\n            location_terms.append("top")\r\n        elif det_y > 2 * img_h / 3:\r\n            location_terms.append("bottom")\r\n\r\n        # Check if VLM description mentions these locations\r\n        vlm_lower = vlm_description.lower()\r\n        return any(term in vlm_lower for term in location_terms)\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"limitations",children:"VLM Limitations"}),"\n",(0,i.jsx)(n.h3,{id:"hallucination-examples",children:"Hallucination Examples"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Query"}),(0,i.jsx)(n.th,{children:"VLM Response"}),(0,i.jsx)(n.th,{children:"Reality"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:'"How many mugs?"'}),(0,i.jsx)(n.td,{children:'"There are 3 mugs"'}),(0,i.jsx)(n.td,{children:"Actually 2"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:'"Is there a cat?"'}),(0,i.jsx)(n.td,{children:'"Yes, on the left"'}),(0,i.jsx)(n.td,{children:"No cat present"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:'"What color is the cup?"'}),(0,i.jsx)(n.td,{children:'"Blue"'}),(0,i.jsx)(n.td,{children:"Actually white"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"reliability-evaluation",children:"Reliability Evaluation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def evaluate_vlm_reliability(vlm, test_set):\r\n    """Evaluate VLM accuracy on robotics queries."""\r\n    results = {\r\n        "object_presence": [],\r\n        "counting": [],\r\n        "color_accuracy": [],\r\n        "spatial_accuracy": [],\r\n    }\r\n\r\n    for image, ground_truth in test_set:\r\n        # Object presence\r\n        for obj in ground_truth["objects"]:\r\n            response, _ = vlm.query(image, f"Is there a {obj[\'name\']} in this image?")\r\n            correct = "yes" in response.lower()\r\n            results["object_presence"].append(correct)\r\n\r\n        # Counting\r\n        for category, count in ground_truth["counts"].items():\r\n            response, _ = vlm.query(image, f"How many {category}s are there?")\r\n            # Parse number from response\r\n            predicted = extract_number(response)\r\n            results["counting"].append(predicted == count)\r\n\r\n    return {k: np.mean(v) for k, v in results.items()}\n'})}),"\n",(0,i.jsx)(n.p,{children:"Expected results:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Task"}),(0,i.jsx)(n.th,{children:"Typical Accuracy"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Object presence"}),(0,i.jsx)(n.td,{children:"80-90%"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Counting (\u22643)"}),(0,i.jsx)(n.td,{children:"70-85%"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Counting (>3)"}),(0,i.jsx)(n.td,{children:"40-60%"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Color"}),(0,i.jsx)(n.td,{children:"75-90%"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Spatial relations"}),(0,i.jsx)(n.td,{children:"60-80%"})]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"latency",children:"Latency Considerations"}),"\n",(0,i.jsx)(n.h3,{id:"vlm-latency-in-vla-pipeline",children:"VLM Latency in VLA Pipeline"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Speech (200ms) + LLM (500ms) + VLM (2000ms) + Grounding (50ms) = 2750ms\r\n                                    \u2191\r\n                              Dominates!\n"})}),"\n",(0,i.jsx)(n.h3,{id:"latency-mitigation",children:"Latency Mitigation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class CachedVLM:\r\n    def __init__(self, vlm):\r\n        self.vlm = vlm\r\n        self.scene_cache = {}  # image_hash -> scene description\r\n        self.cache_timeout = 5.0  # seconds\r\n\r\n    def query(self, image: np.ndarray, question: str) -> tuple[str, float]:\r\n        """Query with scene caching."""\r\n        # For general scene queries, use cache\r\n        if self.is_scene_query(question):\r\n            scene = self.get_or_update_scene(image)\r\n            # Answer from cached scene description\r\n            return self.answer_from_scene(scene, question), 0\r\n\r\n        # For specific queries, call VLM\r\n        return self.vlm.query(image, question)\r\n\r\n    def get_or_update_scene(self, image: np.ndarray) -> dict:\r\n        """Get cached scene or update."""\r\n        image_hash = self.hash_image(image)\r\n\r\n        if image_hash in self.scene_cache:\r\n            cached = self.scene_cache[image_hash]\r\n            if time.time() - cached[\'time\'] < self.cache_timeout:\r\n                return cached[\'scene\']\r\n\r\n        # Generate new scene description\r\n        scene_response, _ = self.vlm.query(\r\n            image,\r\n            "Describe all objects in this image with their colors and positions."\r\n        )\r\n\r\n        self.scene_cache[image_hash] = {\r\n            \'scene\': self.parse_scene(scene_response),\r\n            \'time\': time.time()\r\n        }\r\n\r\n        return self.scene_cache[image_hash][\'scene\']\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"This chapter covered VLMs for robot scene understanding:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"VLM capabilities"})," complement object detection with flexible queries and spatial reasoning."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Deployment options"})," include local (LLaVA) and API (GPT-4V) with different latency/quality tradeoffs."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Prompt engineering"})," extracts robotics-relevant information (objects, locations, relations)."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Integration with grounding"})," uses VLM to resolve ambiguous references."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Limitations"})," include hallucination and counting errors\u2014VLMs are not reliable for safety-critical perception."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Latency management"})," via caching helps with real-time constraints."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"safety-callout",children:"Safety Callout"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Never trust VLM for safety-critical decisions:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:'VLM: "The path is clear" \u2192 Might hallucinate'}),"\n",(0,i.jsx)(n.li,{children:'VLM: "No humans in frame" \u2192 Could miss person'}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Use traditional perception (LiDAR, depth) for safety. VLM assists with understanding, not safety verification."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"self-assessment-questions",children:"Self-Assessment Questions"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"VLM vs Detection"}),": When would you use VLM instead of object detection? When detection instead of VLM?"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Hallucination Risk"}),': You ask "Is there a person?" and VLM says "No". The robot proceeds and hits a person. What went wrong? How would you prevent this?']}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Latency Trade-off"}),": VLM adds 2 seconds to response time. When is this acceptable? When unacceptable?"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Prompt Design"}),': Design a prompt to identify "the thing the user is holding" from a camera image.']}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Integration"}),': Your grounding layer resolved to the wrong object because VLM said it was "on the left" when it was actually center-left. How would you improve matching accuracy?']}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"whats-next",children:"What's Next"}),"\n",(0,i.jsxs)(n.p,{children:["In ",(0,i.jsx)(n.a,{href:"/module-5/chapter-5-safety-constraints",children:"Chapter 5: Safety Constraints"}),", you'll implement the critical safety layer that prevents dangerous AI-generated commands from reaching robot actuators."]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453(e,n,r){r.d(n,{R:()=>l,x:()=>o});var t=r(6540);const i={},s=t.createContext(i);function l(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);