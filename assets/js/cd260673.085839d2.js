"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[3814],{3510(e,n,r){r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-5/chapter-3-grounding","title":"Chapter 3: Grounding","description":"Chapter Goal","source":"@site/docs/module-5/chapter-3-grounding.md","sourceDirName":"module-5","slug":"/module-5/chapter-3-grounding","permalink":"/Master-Robotics-AI/textbook/module-5/chapter-3-grounding","draft":false,"unlisted":false,"editUrl":"https://github.com/HasnainCodeHub/Master-Robotics-AI/tree/main/docs/docs/module-5/chapter-3-grounding.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"id":"chapter-3-grounding","title":"Chapter 3: Grounding","sidebar_label":"3. Grounding","sidebar_position":4},"sidebar":"textbookSidebar","previous":{"title":"2. LLM Task Planning","permalink":"/Master-Robotics-AI/textbook/module-5/chapter-2-llm-task-planning"},"next":{"title":"4. Vision-Language Models","permalink":"/Master-Robotics-AI/textbook/module-5/chapter-4-vision-language-models"}}');var o=r(4848),i=r(8453);const s={id:"chapter-3-grounding",title:"Chapter 3: Grounding",sidebar_label:"3. Grounding",sidebar_position:4},a="Chapter 3: Grounding and Symbol-to-Action Mapping",l={},c=[{value:"Chapter Goal",id:"chapter-goal",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"The Symbol Grounding Problem",id:"problem",level:2},{value:"Why Can&#39;t Robots Just Execute Plans?",id:"why-cant-robots-just-execute-plans",level:3},{value:"Grounding Pipeline",id:"grounding-pipeline",level:3},{value:"Object Reference Resolution",id:"object-resolution",level:2},{value:"From Name to Detection",id:"from-name-to-detection",level:3},{value:"Handling Ambiguity",id:"handling-ambiguity",level:3},{value:"Spatial Reference Resolution",id:"spatial-resolution",level:2},{value:"Location Descriptions to Coordinates",id:"location-descriptions-to-coordinates",level:3},{value:"Action Mapping",id:"action-mapping",level:2},{value:"From Symbolic Action to ROS 2 Goal",id:"from-symbolic-action-to-ros-2-goal",level:3},{value:"Grounding Verification",id:"verification",level:2},{value:"Precondition Checking",id:"precondition-checking",level:3},{value:"Handling Grounding Failures",id:"failures",level:2},{value:"Failure Recovery Strategies",id:"failure-recovery-strategies",level:3},{value:"Complete Grounding Node",id:"complete-node",level:2},{value:"Summary",id:"summary",level:2},{value:"Self-Assessment Questions",id:"self-assessment-questions",level:2},{value:"What&#39;s Next",id:"whats-next",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-3-grounding-and-symbol-to-action-mapping",children:"Chapter 3: Grounding and Symbol-to-Action Mapping"})}),"\n",(0,o.jsx)(n.h2,{id:"chapter-goal",children:"Chapter Goal"}),"\n",(0,o.jsxs)(n.p,{children:["By the end of this chapter, you will be able to ",(0,o.jsx)(n.strong,{children:"implement the grounding layer that maps LLM-generated symbolic task plans to executable ROS 2 actions"}),", with explicit scene verification and coordinate frame resolution."]}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"ID"}),(0,o.jsx)(n.th,{children:"Objective"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"3.1"}),(0,o.jsx)(n.td,{children:"Explain the symbol grounding problem"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"3.2"}),(0,o.jsx)(n.td,{children:"Implement object reference resolution"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"3.3"}),(0,o.jsx)(n.td,{children:"Implement spatial reference resolution"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"3.4"}),(0,o.jsx)(n.td,{children:"Implement action mapping to ROS 2 action goals"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"3.5"}),(0,o.jsx)(n.td,{children:"Implement grounding verification"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"3.6"}),(0,o.jsx)(n.td,{children:"Handle grounding failures"})]})]})]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"problem",children:"The Symbol Grounding Problem"}),"\n",(0,o.jsx)(n.h3,{id:"why-cant-robots-just-execute-plans",children:"Why Can't Robots Just Execute Plans?"}),"\n",(0,o.jsx)(n.p,{children:'Consider the command: "Pick up the red mug"'}),"\n",(0,o.jsx)(n.p,{children:"The LLM produces:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'{"action_type": "pick", "target": "red_mug"}\n'})}),"\n",(0,o.jsx)(n.p,{children:"But the robot needs:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"target_pose:\r\n  position: {x: 1.23, y: 0.45, z: 0.82}\r\n  orientation: {x: 0, y: 0, z: 0, w: 1}\r\napproach_direction: {x: 0, y: 0, z: -1}\n"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Symbol grounding"}),' connects abstract symbols ("red_mug") to physical quantities (pose, reachability, grasp points).']}),"\n",(0,o.jsx)(n.h3,{id:"grounding-pipeline",children:"Grounding Pipeline"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Symbolic Plan \u2500\u2500\u25ba Object Resolution \u2500\u2500\u25ba Spatial Resolution \u2500\u2500\u25ba Action Mapping\r\n     \u2502                    \u2502                     \u2502                    \u2502\r\n"red_mug"        Detection + ID        Pose estimation        ROS 2 Action\r\n                                                                  Goal\n'})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"object-resolution",children:"Object Reference Resolution"}),"\n",(0,o.jsx)(n.h3,{id:"from-name-to-detection",children:"From Name to Detection"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""Object reference resolution using perception."""\r\n\r\nfrom dataclasses import dataclass\r\nfrom typing import Optional, List\r\nimport numpy as np\r\n\r\n\r\n@dataclass\r\nclass DetectedObject:\r\n    class_name: str\r\n    instance_id: int\r\n    confidence: float\r\n    bbox_2d: tuple  # (x, y, width, height)\r\n    position_3d: np.ndarray  # (x, y, z) in robot frame\r\n    color: Optional[str] = None\r\n\r\n\r\nclass ObjectResolver:\r\n    def __init__(self):\r\n        self.detections: List[DetectedObject] = []\r\n\r\n    def update_detections(self, detection_msg):\r\n        """Update from perception pipeline."""\r\n        self.detections = []\r\n        for det in detection_msg.detections:\r\n            obj = DetectedObject(\r\n                class_name=det.class_name,\r\n                instance_id=det.id,\r\n                confidence=det.confidence,\r\n                bbox_2d=(det.bbox.x, det.bbox.y, det.bbox.w, det.bbox.h),\r\n                position_3d=np.array([det.pose.x, det.pose.y, det.pose.z]),\r\n                color=det.color if hasattr(det, \'color\') else None\r\n            )\r\n            self.detections.append(obj)\r\n\r\n    def resolve(self, reference: str) -> Optional[DetectedObject]:\r\n        """\r\n        Resolve textual reference to detected object.\r\n\r\n        Args:\r\n            reference: Object description (e.g., "red_mug", "the box")\r\n\r\n        Returns:\r\n            DetectedObject if found and unambiguous, None otherwise\r\n        """\r\n        # Parse reference\r\n        tokens = reference.lower().replace(\'_\', \' \').split()\r\n\r\n        # Extract color and class\r\n        colors = {\'red\', \'blue\', \'green\', \'yellow\', \'black\', \'white\'}\r\n        ref_color = None\r\n        ref_class = None\r\n\r\n        for token in tokens:\r\n            if token in colors:\r\n                ref_color = token\r\n            elif token not in {\'the\', \'a\', \'an\'}:\r\n                ref_class = token\r\n\r\n        # Find matching objects\r\n        candidates = []\r\n        for det in self.detections:\r\n            # Class match\r\n            if ref_class and ref_class not in det.class_name.lower():\r\n                continue\r\n\r\n            # Color match (if specified)\r\n            if ref_color and det.color != ref_color:\r\n                continue\r\n\r\n            candidates.append(det)\r\n\r\n        # Handle results\r\n        if len(candidates) == 0:\r\n            return None  # Not found\r\n        elif len(candidates) == 1:\r\n            return candidates[0]  # Unambiguous\r\n        else:\r\n            # Ambiguous - return None, caller should request clarification\r\n            return None\r\n\r\n    def get_ambiguous_candidates(self, reference: str) -> List[DetectedObject]:\r\n        """Get all candidates for ambiguous reference."""\r\n        # Same logic as resolve, but return all matches\r\n        # Used for generating clarification requests\r\n        pass\n'})}),"\n",(0,o.jsx)(n.h3,{id:"handling-ambiguity",children:"Handling Ambiguity"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'def resolve_with_disambiguation(\r\n    self,\r\n    reference: str,\r\n    context: dict\r\n) -> tuple[Optional[DetectedObject], Optional[str]]:\r\n    """\r\n    Resolve reference, returning clarification request if ambiguous.\r\n\r\n    Returns:\r\n        (object, None) if resolved\r\n        (None, clarification_question) if ambiguous\r\n        (None, None) if not found\r\n    """\r\n    candidates = self.find_candidates(reference)\r\n\r\n    if len(candidates) == 0:\r\n        return None, None\r\n\r\n    if len(candidates) == 1:\r\n        return candidates[0], None\r\n\r\n    # Ambiguous - generate clarification\r\n    descriptions = [self.describe_object(c) for c in candidates]\r\n    question = f"I see multiple objects matching \'{reference}\': {\', \'.join(descriptions)}. Which one do you mean?"\r\n\r\n    return None, question\n'})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"spatial-resolution",children:"Spatial Reference Resolution"}),"\n",(0,o.jsx)(n.h3,{id:"location-descriptions-to-coordinates",children:"Location Descriptions to Coordinates"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class SpatialResolver:\r\n    def __init__(self):\r\n        self.known_locations = {}  # name -> pose\r\n        self.scene_objects = []\r\n\r\n    def register_location(self, name: str, pose: np.ndarray):\r\n        """Register named location."""\r\n        self.known_locations[name.lower()] = pose\r\n\r\n    def resolve_location(\r\n        self,\r\n        description: str,\r\n        reference_object: Optional[DetectedObject] = None\r\n    ) -> Optional[np.ndarray]:\r\n        """\r\n        Resolve location description to coordinates.\r\n\r\n        Examples:\r\n            "on the shelf" -> shelf surface position\r\n            "next to the keyboard" -> position adjacent to keyboard\r\n            "home position" -> registered home pose\r\n        """\r\n        desc_lower = description.lower()\r\n\r\n        # Check registered locations\r\n        for name, pose in self.known_locations.items():\r\n            if name in desc_lower:\r\n                return pose\r\n\r\n        # Parse spatial relations\r\n        if "on" in desc_lower or "on top of" in desc_lower:\r\n            return self.resolve_on_surface(desc_lower)\r\n        elif "next to" in desc_lower or "beside" in desc_lower:\r\n            return self.resolve_adjacent(desc_lower, reference_object)\r\n        elif "in front of" in desc_lower:\r\n            return self.resolve_relative(desc_lower, "front")\r\n        elif "behind" in desc_lower:\r\n            return self.resolve_relative(desc_lower, "behind")\r\n        elif "left of" in desc_lower:\r\n            return self.resolve_relative(desc_lower, "left")\r\n        elif "right of" in desc_lower:\r\n            return self.resolve_relative(desc_lower, "right")\r\n\r\n        return None\r\n\r\n    def resolve_on_surface(self, description: str) -> Optional[np.ndarray]:\r\n        """Resolve \'on the X\' to surface position."""\r\n        # Find surface object (table, shelf, etc.)\r\n        surfaces = [\'table\', \'shelf\', \'desk\', \'counter\']\r\n        for surface in surfaces:\r\n            if surface in description:\r\n                for obj in self.scene_objects:\r\n                    if surface in obj.class_name.lower():\r\n                        # Return position on top of surface\r\n                        pos = obj.position_3d.copy()\r\n                        pos[2] += 0.1  # Above surface\r\n                        return pos\r\n        return None\n'})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"action-mapping",children:"Action Mapping"}),"\n",(0,o.jsx)(n.h3,{id:"from-symbolic-action-to-ros-2-goal",children:"From Symbolic Action to ROS 2 Goal"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"Map symbolic actions to ROS 2 action goals.\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.action import ActionClient\r\nfrom geometry_msgs.msg import Pose, PoseStamped\r\nfrom moveit_msgs.action import MoveGroup\r\nfrom nav2_msgs.action import NavigateToPose\r\n\r\n\r\nclass ActionMapper:\r\n    def __init__(self, node):\r\n        self.node = node\r\n\r\n        # Action clients\r\n        self.nav_client = ActionClient(\r\n            node, NavigateToPose, '/navigate_to_pose'\r\n        )\r\n        self.moveit_client = ActionClient(\r\n            node, MoveGroup, '/move_action'\r\n        )\r\n\r\n    def map_action(\r\n        self,\r\n        action: dict,\r\n        grounded_target: Optional[DetectedObject],\r\n        grounded_position: Optional[np.ndarray]\r\n    ) -> tuple[str, Any]:\r\n        \"\"\"\r\n        Map symbolic action to ROS 2 action goal.\r\n\r\n        Returns:\r\n            (action_name, goal_msg)\r\n        \"\"\"\r\n        action_type = action['action_type']\r\n\r\n        if action_type == 'navigate_to':\r\n            return self.map_navigate(action, grounded_position)\r\n\r\n        elif action_type == 'pick':\r\n            return self.map_pick(action, grounded_target)\r\n\r\n        elif action_type == 'place':\r\n            return self.map_place(action, grounded_position)\r\n\r\n        elif action_type == 'open_gripper':\r\n            return self.map_gripper(open=True)\r\n\r\n        elif action_type == 'close_gripper':\r\n            force = action.get('parameters', {}).get('force', 50.0)\r\n            return self.map_gripper(open=False, force=force)\r\n\r\n        else:\r\n            raise ValueError(f\"Unknown action type: {action_type}\")\r\n\r\n    def map_navigate(\r\n        self,\r\n        action: dict,\r\n        position: np.ndarray\r\n    ) -> tuple[str, NavigateToPose.Goal]:\r\n        \"\"\"Map navigate action to Nav2 goal.\"\"\"\r\n        goal = NavigateToPose.Goal()\r\n        goal.pose.header.frame_id = 'map'\r\n        goal.pose.pose.position.x = float(position[0])\r\n        goal.pose.pose.position.y = float(position[1])\r\n        goal.pose.pose.position.z = 0.0\r\n        goal.pose.pose.orientation.w = 1.0\r\n\r\n        return 'navigate_to_pose', goal\r\n\r\n    def map_pick(\r\n        self,\r\n        action: dict,\r\n        target: DetectedObject\r\n    ) -> tuple[str, MoveGroup.Goal]:\r\n        \"\"\"Map pick action to MoveIt goal.\"\"\"\r\n        goal = MoveGroup.Goal()\r\n\r\n        # Set target pose (pre-grasp position above object)\r\n        target_pose = PoseStamped()\r\n        target_pose.header.frame_id = 'base_link'\r\n        target_pose.pose.position.x = float(target.position_3d[0])\r\n        target_pose.pose.position.y = float(target.position_3d[1])\r\n        target_pose.pose.position.z = float(target.position_3d[2] + 0.1)\r\n\r\n        # Top-down grasp orientation\r\n        target_pose.pose.orientation.x = 0.707\r\n        target_pose.pose.orientation.y = 0.707\r\n        target_pose.pose.orientation.z = 0.0\r\n        target_pose.pose.orientation.w = 0.0\r\n\r\n        # Configure MoveIt goal\r\n        goal.request.group_name = 'arm'\r\n        goal.request.goal_constraints = [\r\n            self.create_pose_constraint(target_pose)\r\n        ]\r\n\r\n        return 'move_group', goal\n"})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"verification",children:"Grounding Verification"}),"\n",(0,o.jsx)(n.h3,{id:"precondition-checking",children:"Precondition Checking"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class GroundingVerifier:\r\n    def __init__(self, node, object_resolver, spatial_resolver):\r\n        self.node = node\r\n        self.object_resolver = object_resolver\r\n        self.spatial_resolver = spatial_resolver\r\n\r\n    def verify_action(\r\n        self,\r\n        action: dict,\r\n        grounded_target: Optional[DetectedObject],\r\n        grounded_position: Optional[np.ndarray]\r\n    ) -> tuple[bool, str]:\r\n        """\r\n        Verify action preconditions are met.\r\n\r\n        Returns:\r\n            (success, error_message)\r\n        """\r\n        action_type = action[\'action_type\']\r\n\r\n        if action_type == \'pick\':\r\n            return self.verify_pick(action, grounded_target)\r\n\r\n        elif action_type == \'place\':\r\n            return self.verify_place(action, grounded_position)\r\n\r\n        elif action_type == \'navigate_to\':\r\n            return self.verify_navigation(action, grounded_position)\r\n\r\n        return True, ""\r\n\r\n    def verify_pick(\r\n        self,\r\n        action: dict,\r\n        target: Optional[DetectedObject]\r\n    ) -> tuple[bool, str]:\r\n        """Verify pick preconditions."""\r\n\r\n        # Target must be resolved\r\n        if target is None:\r\n            return False, "Target object not found"\r\n\r\n        # Target must be visible (recent detection)\r\n        if not self.is_currently_visible(target):\r\n            return False, "Target not currently visible"\r\n\r\n        # Target must be reachable\r\n        if not self.is_reachable(target.position_3d):\r\n            return False, "Target position not reachable"\r\n\r\n        # Gripper must be empty\r\n        if self.is_gripper_holding():\r\n            return False, "Gripper is not empty"\r\n\r\n        return True, ""\r\n\r\n    def is_reachable(self, position: np.ndarray) -> bool:\r\n        """Check if position is within robot workspace."""\r\n        # Simple workspace check\r\n        x, y, z = position\r\n        return (0.2 <= x <= 0.8 and\r\n                -0.5 <= y <= 0.5 and\r\n                0.0 <= z <= 1.0)\r\n\r\n    def is_currently_visible(self, target: DetectedObject) -> bool:\r\n        """Check if target was detected recently."""\r\n        current_detections = self.object_resolver.detections\r\n        for det in current_detections:\r\n            if det.instance_id == target.instance_id:\r\n                return True\r\n        return False\n'})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"failures",children:"Handling Grounding Failures"}),"\n",(0,o.jsx)(n.h3,{id:"failure-recovery-strategies",children:"Failure Recovery Strategies"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class GroundingFailureHandler:\r\n    def __init__(self, node):\r\n        self.node = node\r\n        self.clarification_pub = node.create_publisher(\r\n            String, \'/vla/clarification_request\', 10\r\n        )\r\n\r\n    def handle_failure(\r\n        self,\r\n        action: dict,\r\n        failure_reason: str,\r\n        context: dict\r\n    ) -> Optional[dict]:\r\n        """\r\n        Handle grounding failure with appropriate recovery.\r\n\r\n        Returns:\r\n            Recovery action or None if unrecoverable\r\n        """\r\n        if "not found" in failure_reason.lower():\r\n            return self.handle_not_found(action, context)\r\n\r\n        elif "ambiguous" in failure_reason.lower():\r\n            return self.handle_ambiguous(action, context)\r\n\r\n        elif "not reachable" in failure_reason.lower():\r\n            return self.handle_unreachable(action, context)\r\n\r\n        elif "not visible" in failure_reason.lower():\r\n            return self.handle_not_visible(action, context)\r\n\r\n        return None\r\n\r\n    def handle_not_found(self, action: dict, context: dict) -> dict:\r\n        """Object not found - request human assistance."""\r\n        target = action.get(\'target\', \'the object\')\r\n\r\n        self.request_clarification(\r\n            f"I cannot find {target}. Can you show me where it is?"\r\n        )\r\n\r\n        return {\r\n            "action": "wait_for_assistance",\r\n            "timeout": 30.0,\r\n            "then_retry": True\r\n        }\r\n\r\n    def handle_ambiguous(self, action: dict, context: dict) -> dict:\r\n        """Multiple candidates - request clarification."""\r\n        candidates = context.get(\'candidates\', [])\r\n        descriptions = [f"{c.color} {c.class_name}" for c in candidates]\r\n\r\n        self.request_clarification(\r\n            f"I see multiple objects: {\', \'.join(descriptions)}. "\r\n            "Which one should I pick?"\r\n        )\r\n\r\n        return {\r\n            "action": "wait_for_clarification",\r\n            "timeout": 30.0\r\n        }\r\n\r\n    def handle_unreachable(self, action: dict, context: dict) -> dict:\r\n        """Position not reachable - navigate closer."""\r\n        target_pos = context.get(\'target_position\')\r\n\r\n        return {\r\n            "action": "navigate_closer",\r\n            "target": target_pos,\r\n            "then_retry": True\r\n        }\r\n\r\n    def request_clarification(self, message: str):\r\n        """Publish clarification request (for speech synthesis)."""\r\n        msg = String()\r\n        msg.data = message\r\n        self.clarification_pub.publish(msg)\r\n        self.node.get_logger().info(f"Requesting clarification: {message}")\n'})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"complete-node",children:"Complete Grounding Node"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"Complete grounding node connecting LLM planner to robot execution.\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nimport json\r\n\r\n\r\nclass GroundingNode(Node):\r\n    def __init__(self):\r\n        super().__init__('grounding')\r\n\r\n        # Components\r\n        self.object_resolver = ObjectResolver()\r\n        self.spatial_resolver = SpatialResolver()\r\n        self.action_mapper = ActionMapper(self)\r\n        self.verifier = GroundingVerifier(\r\n            self, self.object_resolver, self.spatial_resolver\r\n        )\r\n        self.failure_handler = GroundingFailureHandler(self)\r\n\r\n        # Subscriptions\r\n        self.plan_sub = self.create_subscription(\r\n            String, '/task_planner/plan', self.plan_callback, 10\r\n        )\r\n        self.detection_sub = self.create_subscription(\r\n            DetectionArray, '/detections', self.detection_callback, 10\r\n        )\r\n\r\n        # Publishers\r\n        self.action_pub = self.create_publisher(\r\n            String, '/grounding/action', 10\r\n        )\r\n\r\n        self.get_logger().info('Grounding node ready')\r\n\r\n    def plan_callback(self, msg: String):\r\n        \"\"\"Process task plan and ground each action.\"\"\"\r\n        plan = json.loads(msg.data)\r\n\r\n        for action in plan.get('actions', []):\r\n            grounded = self.ground_action(action)\r\n\r\n            if grounded is None:\r\n                self.get_logger().error(f\"Failed to ground action: {action}\")\r\n                continue\r\n\r\n            # Publish grounded action\r\n            action_msg = String()\r\n            action_msg.data = json.dumps(grounded)\r\n            self.action_pub.publish(action_msg)\r\n\r\n    def ground_action(self, action: dict) -> Optional[dict]:\r\n        \"\"\"Ground single action to executable form.\"\"\"\r\n        # Resolve object reference\r\n        target = None\r\n        if action.get('target'):\r\n            target = self.object_resolver.resolve(action['target'])\r\n\r\n        # Resolve spatial reference\r\n        position = None\r\n        if action.get('parameters', {}).get('position'):\r\n            pos_ref = action['parameters']['position']\r\n            if isinstance(pos_ref, str):\r\n                position = self.spatial_resolver.resolve_location(pos_ref)\r\n            else:\r\n                position = np.array(pos_ref)\r\n\r\n        # Verify preconditions\r\n        success, error = self.verifier.verify_action(action, target, position)\r\n\r\n        if not success:\r\n            recovery = self.failure_handler.handle_failure(\r\n                action, error, {'target': target, 'position': position}\r\n            )\r\n            return recovery\r\n\r\n        # Map to ROS 2 action\r\n        action_name, goal = self.action_mapper.map_action(\r\n            action, target, position\r\n        )\r\n\r\n        return {\r\n            'action_name': action_name,\r\n            'goal': self.goal_to_dict(goal),\r\n            'original_action': action\r\n        }\n"})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"This chapter covered grounding\u2014connecting symbols to the physical world:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Symbol grounding"}),' maps abstract references ("red_mug") to physical quantities (pose).']}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Object resolution"})," uses perception to find and identify referenced objects."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Spatial resolution"})," converts location descriptions to coordinates."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Action mapping"})," translates symbolic actions to ROS 2 action goals."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Verification"})," checks preconditions before execution."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Failure handling"})," requests clarification or triggers recovery."]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"self-assessment-questions",children:"Self-Assessment Questions"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Grounding Challenge"}),': The LLM says "pick up the cup" but perception detects "mug" not "cup". How would you handle this synonym problem?']}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Spatial Ambiguity"}),': "Put it on the table" when there are two tables. What disambiguation strategy would you use?']}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Verification Design"}),': What preconditions would you verify for a "place" action?']}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Failure Recovery"}),": The target object disappears during approach (someone moved it). What should the system do?"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Coordinate Frames"}),": Your perception gives positions in camera_link frame but navigation needs map frame. What's needed for grounding?"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"whats-next",children:"What's Next"}),"\n",(0,o.jsxs)(n.p,{children:["In ",(0,o.jsx)(n.a,{href:"/module-5/chapter-4-vision-language-models",children:"Chapter 4: Vision-Language Models"}),", you'll integrate VLMs for scene understanding and flexible object identification."]})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453(e,n,r){r.d(n,{R:()=>s,x:()=>a});var t=r(6540);const o={},i=t.createContext(o);function s(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);