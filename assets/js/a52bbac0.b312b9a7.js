"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[4719],{5968(e,r,n){n.r(r),n.d(r,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-4/chapter-2-scene-composition-sensors","title":"Chapter 2: Scene Composition and Sensors","description":"Chapter Goal","source":"@site/docs/module-4/chapter-2-scene-composition-sensors.md","sourceDirName":"module-4","slug":"/module-4/chapter-2-scene-composition-sensors","permalink":"/Master-Robotics-AI/textbook/module-4/chapter-2-scene-composition-sensors","draft":false,"unlisted":false,"editUrl":"https://github.com/HasnainCodeHub/Master-Robotics-AI/tree/main/docs/docs/module-4/chapter-2-scene-composition-sensors.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"id":"chapter-2-scene-composition-sensors","title":"Chapter 2: Scene Composition and Sensors","sidebar_label":"2. Scenes & Sensors","sidebar_position":3},"sidebar":"textbookSidebar","previous":{"title":"1. Isaac Sim Foundations","permalink":"/Master-Robotics-AI/textbook/module-4/chapter-1-isaac-sim-foundations"},"next":{"title":"3. Isaac ROS","permalink":"/Master-Robotics-AI/textbook/module-4/chapter-3-isaac-ros-integration"}}');var s=n(4848),a=n(8453);const t={id:"chapter-2-scene-composition-sensors",title:"Chapter 2: Scene Composition and Sensors",sidebar_label:"2. Scenes & Sensors",sidebar_position:3},o="Chapter 2: Isaac Sim Scene Composition and Sensors",l={},d=[{value:"Chapter Goal",id:"chapter-goal",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"From SDF to USD: Composition Paradigms",id:"from-sdf-to-usd-composition-paradigms",level:2},{value:"USD Composition Operations",id:"usd-composition-operations",level:3},{value:"Practical Composition Example",id:"practical-composition-example",level:3},{value:"URDF Import",id:"urdf-import",level:2},{value:"Import Process",id:"import-process",level:3},{value:"Import Settings",id:"import-settings",level:3},{value:"Verifying Import",id:"verifying-import",level:3},{value:"RTX Camera Configuration",id:"camera",level:2},{value:"Creating a Camera",id:"creating-a-camera",level:3},{value:"Camera Parameters",id:"camera-parameters",level:3},{value:"Depth Camera Configuration",id:"depth-camera-configuration",level:3},{value:"Camera Noise",id:"camera-noise",level:3},{value:"RTX LiDAR Configuration",id:"lidar",level:2},{value:"RTX LiDAR Advantages",id:"rtx-lidar-advantages",level:3},{value:"Configuration",id:"configuration",level:3},{value:"LiDAR Noise Configuration",id:"lidar-noise-configuration",level:3},{value:"Getting LiDAR Data",id:"getting-lidar-data",level:3},{value:"IMU Configuration",id:"imu",level:2},{value:"Creating IMU Sensor",id:"creating-imu-sensor",level:3},{value:"IMU Noise Parameters",id:"imu-noise-parameters",level:3},{value:"Getting IMU Data",id:"getting-imu-data",level:3},{value:"Ground Truth Extraction",id:"ground-truth",level:2},{value:"Pose Ground Truth",id:"pose-ground-truth",level:3},{value:"Sensor Ground Truth",id:"sensor-ground-truth",level:3},{value:"Object Annotations",id:"object-annotations",level:3},{value:"Complete Sensor Suite Example",id:"complete-example",level:2},{value:"Summary",id:"summary",level:2},{value:"Reality Gap Callout",id:"reality-gap-callout",level:2},{value:"Self-Assessment Questions",id:"self-assessment-questions",level:2},{value:"What&#39;s Next",id:"whats-next",level:2}];function c(e){const r={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(r.header,{children:(0,s.jsx)(r.h1,{id:"chapter-2-isaac-sim-scene-composition-and-sensors",children:"Chapter 2: Isaac Sim Scene Composition and Sensors"})}),"\n",(0,s.jsx)(r.h2,{id:"chapter-goal",children:"Chapter Goal"}),"\n",(0,s.jsxs)(r.p,{children:["By the end of this chapter, you will be able to ",(0,s.jsx)(r.strong,{children:"master USD-based scene composition in Isaac Sim and configure RTX-accelerated sensors"})," with physically-based rendering for photorealistic synthetic data."]}),"\n",(0,s.jsx)(r.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(r.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,s.jsxs)(r.table,{children:[(0,s.jsx)(r.thead,{children:(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.th,{children:"ID"}),(0,s.jsx)(r.th,{children:"Objective"})]})}),(0,s.jsxs)(r.tbody,{children:[(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"2.1"}),(0,s.jsx)(r.td,{children:"Create Isaac Sim scenes using USD layers and references"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"2.2"}),(0,s.jsx)(r.td,{children:"Import URDF robots into Isaac Sim and configure articulation physics"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"2.3"}),(0,s.jsx)(r.td,{children:"Configure RGB and depth cameras with RTX rendering"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"2.4"}),(0,s.jsx)(r.td,{children:"Configure LiDAR sensors with RTX ray-tracing"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"2.5"}),(0,s.jsx)(r.td,{children:"Configure IMU sensors with realistic noise parameters"})]})]})]}),"\n",(0,s.jsx)(r.hr,{}),"\n",(0,s.jsx)(r.h2,{id:"from-sdf-to-usd-composition-paradigms",children:"From SDF to USD: Composition Paradigms"}),"\n",(0,s.jsxs)(r.p,{children:["In Module 3, you used SDF's ",(0,s.jsx)(r.code,{children:"<include>"})," tags for composition. USD provides a more powerful model."]}),"\n",(0,s.jsx)(r.h3,{id:"usd-composition-operations",children:"USD Composition Operations"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{children:"USD Composition Stack (strongest to weakest):\r\n1. Local opinions        \u2190 Edits in the current layer\r\n2. Sublayers            \u2190 Layers stacked on top\r\n3. References           \u2190 Include other USD files\r\n4. Payloads             \u2190 Lazy-loaded references\r\n5. Variants             \u2190 Switchable alternatives\r\n6. Inherits             \u2190 Class-like inheritance\r\n7. Specializes          \u2190 Derived types\n"})}),"\n",(0,s.jsx)(r.h3,{id:"practical-composition-example",children:"Practical Composition Example"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'# Python USD composition in Isaac Sim\r\nfrom pxr import Usd, UsdGeom, Sdf\r\n\r\n# Create new stage\r\nstage = Usd.Stage.CreateNew("warehouse_scene.usd")\r\n\r\n# Set up scene\r\nUsdGeom.SetStageUpAxis(stage, UsdGeom.Tokens.z)\r\nUsdGeom.SetStageMetersPerUnit(stage, 1.0)\r\n\r\n# Add reference to robot\r\nrobot_prim = stage.DefinePrim("/World/Robot")\r\nrobot_prim.GetReferences().AddReference("./robot.usd")\r\n\r\n# Add reference to environment\r\nwarehouse_prim = stage.DefinePrim("/World/Warehouse")\r\nwarehouse_prim.GetReferences().AddReference("./warehouse_env.usd")\r\n\r\n# Override robot position\r\nxform = UsdGeom.Xformable(robot_prim)\r\nxform.AddTranslateOp().Set((5.0, 3.0, 0.0))\r\n\r\nstage.Save()\n'})}),"\n",(0,s.jsx)(r.hr,{}),"\n",(0,s.jsx)(r.h2,{id:"urdf-import",children:"URDF Import"}),"\n",(0,s.jsx)(r.h3,{id:"import-process",children:"Import Process"}),"\n",(0,s.jsx)(r.p,{children:"Isaac Sim can import your Module 2 URDF files:"}),"\n",(0,s.jsxs)(r.ol,{children:["\n",(0,s.jsx)(r.li,{children:(0,s.jsx)(r.strong,{children:"File \u2192 Import \u2192 URDF"})}),"\n",(0,s.jsx)(r.li,{children:(0,s.jsx)(r.strong,{children:"Select URDF file"})}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Configure import settings"}),":","\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsx)(r.li,{children:"Fix base link: Yes (for manipulation), No (for mobile)"}),"\n",(0,s.jsx)(r.li,{children:"Self collision: Enable"}),"\n",(0,s.jsx)(r.li,{children:"Create physics: Enable"}),"\n",(0,s.jsx)(r.li,{children:"Joint drive type: Position/Velocity"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"import-settings",children:"Import Settings"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'# Programmatic URDF import\r\nfrom omni.isaac.urdf import _urdf\r\n\r\nurdf_interface = _urdf.acquire_urdf_interface()\r\n\r\nimport_config = _urdf.ImportConfig()\r\nimport_config.merge_fixed_joints = False\r\nimport_config.fix_base = False  # Mobile robot\r\nimport_config.import_inertia_tensor = True\r\nimport_config.self_collision = True\r\nimport_config.default_drive_type = _urdf.UrdfJointTargetType.JOINT_DRIVE_VELOCITY\r\n\r\n# Import the robot\r\nresult = urdf_interface.parse_urdf(\r\n    "robot.urdf",\r\n    import_config\r\n)\r\nrobot_path = urdf_interface.import_robot(\r\n    "/World/Robot",\r\n    result,\r\n    import_config\r\n)\n'})}),"\n",(0,s.jsx)(r.h3,{id:"verifying-import",children:"Verifying Import"}),"\n",(0,s.jsx)(r.p,{children:"After import, verify:"}),"\n",(0,s.jsxs)(r.ol,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Joint limits match URDF"}),": Property panel \u2192 Articulation \u2192 Joint limits"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Mass/inertia correct"}),": Property panel \u2192 Rigid Body \u2192 Mass"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Collision geometry"}),": Viewport \u2192 Show \u2192 Physics Collision"]}),"\n"]}),"\n",(0,s.jsx)(r.hr,{}),"\n",(0,s.jsx)(r.h2,{id:"camera",children:"RTX Camera Configuration"}),"\n",(0,s.jsx)(r.h3,{id:"creating-a-camera",children:"Creating a Camera"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'from omni.isaac.sensor import Camera\r\nimport omni.replicator.core as rep\r\n\r\n# Create camera sensor\r\ncamera = Camera(\r\n    prim_path="/World/Robot/camera_link/camera",\r\n    resolution=(640, 480),\r\n    frequency=30,\r\n    translation=np.array([0.0, 0.0, 0.0]),\r\n    orientation=np.array([1.0, 0.0, 0.0, 0.0])  # Quaternion (w,x,y,z)\r\n)\r\n\r\n# Initialize\r\ncamera.initialize()\n'})}),"\n",(0,s.jsx)(r.h3,{id:"camera-parameters",children:"Camera Parameters"}),"\n",(0,s.jsx)(r.p,{children:"Matching Intel RealSense D435 from Module 3:"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'# Configure camera to match hardware specs\r\ncamera_config = {\r\n    "resolution": (1920, 1080),  # Or (640, 480) for performance\r\n    "frequency": 30,\r\n    "focal_length": 1.93,  # mm, from datasheet\r\n    "horizontal_aperture": 3.6,  # mm (sensor width)\r\n    "vertical_aperture": 2.4,  # mm (sensor height)\r\n    "clipping_range": (0.1, 10.0),  # meters\r\n}\r\n\r\n# Resulting FOV calculation:\r\n# HFOV = 2 * atan(horizontal_aperture / (2 * focal_length))\r\n# HFOV = 2 * atan(3.6 / (2 * 1.93)) = 86.4\xb0 (matches D435 ~87\xb0)\n'})}),"\n",(0,s.jsx)(r.h3,{id:"depth-camera-configuration",children:"Depth Camera Configuration"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'from omni.isaac.sensor import Camera\r\n\r\n# RGB-D camera (like RealSense)\r\nrgbd_camera = Camera(\r\n    prim_path="/World/Robot/camera_link/rgbd_camera",\r\n    resolution=(640, 480),\r\n    frequency=30\r\n)\r\n\r\n# Enable depth output\r\nrgbd_camera.add_distance_to_camera_output()\r\nrgbd_camera.add_distance_to_image_plane_output()\r\n\r\n# Get depth data\r\ndepth_image = rgbd_camera.get_depth_image()\n'})}),"\n",(0,s.jsx)(r.h3,{id:"camera-noise",children:"Camera Noise"}),"\n",(0,s.jsx)(r.p,{children:"RTX cameras support physically-based noise:"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:"# Add post-processing effects for realism\r\nfrom omni.isaac.synthetic_utils import SyntheticDataHelper\r\n\r\n# Motion blur (from robot motion)\r\n# Enabled in render settings\r\n\r\n# Lens distortion\r\n# Configurable in camera intrinsics\r\n\r\n# Exposure/gain noise\r\n# Add in Replicator randomization (Chapter 5)\n"})}),"\n",(0,s.jsx)(r.hr,{}),"\n",(0,s.jsx)(r.h2,{id:"lidar",children:"RTX LiDAR Configuration"}),"\n",(0,s.jsx)(r.h3,{id:"rtx-lidar-advantages",children:"RTX LiDAR Advantages"}),"\n",(0,s.jsxs)(r.table,{children:[(0,s.jsx)(r.thead,{children:(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.th,{children:"Aspect"}),(0,s.jsx)(r.th,{children:"Gazebo LiDAR"}),(0,s.jsx)(r.th,{children:"Isaac RTX LiDAR"})]})}),(0,s.jsxs)(r.tbody,{children:[(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"Method"}),(0,s.jsx)(r.td,{children:"CPU ray-casting"}),(0,s.jsx)(r.td,{children:"GPU ray-tracing"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"Speed"}),(0,s.jsx)(r.td,{children:"Limited rays"}),(0,s.jsx)(r.td,{children:"Millions of rays"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"Accuracy"}),(0,s.jsx)(r.td,{children:"Geometry only"}),(0,s.jsx)(r.td,{children:"Material properties"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"Multi-path"}),(0,s.jsx)(r.td,{children:"Not modeled"}),(0,s.jsx)(r.td,{children:"Partial support"})]})]})]}),"\n",(0,s.jsx)(r.h3,{id:"configuration",children:"Configuration"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'from omni.isaac.sensor import LidarRtx\r\n\r\n# Create RTX LiDAR (Velodyne VLP-16 equivalent)\r\nlidar = LidarRtx(\r\n    prim_path="/World/Robot/lidar_link/lidar"\r\n)\r\n\r\n# Configure to match VLP-16 specifications\r\nlidar.set_fov([360.0, 30.0])  # Horizontal, Vertical FOV\r\nlidar.set_resolution([0.4, 2.0])  # Horizontal, Vertical resolution (degrees)\r\nlidar.set_valid_range([0.5, 100.0])  # Min, Max range (meters)\r\nlidar.set_rotation_frequency(10.0)  # Hz\r\n\r\nlidar.initialize()\n'})}),"\n",(0,s.jsx)(r.h3,{id:"lidar-noise-configuration",children:"LiDAR Noise Configuration"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'# Enable noise model\r\nlidar_config = {\r\n    "min_range": 0.5,\r\n    "max_range": 100.0,\r\n    "noise_type": "gaussian",\r\n    "noise_stddev": 0.02,  # 2cm range noise (from M3 datasheet)\r\n    "dropout_rate": 0.01,  # 1% dropout\r\n}\n'})}),"\n",(0,s.jsx)(r.h3,{id:"getting-lidar-data",children:"Getting LiDAR Data"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'# Get point cloud\r\npoint_cloud = lidar.get_current_frame()\r\n\r\n# Data format: (N, 3) array of XYZ points\r\n# In robot\'s lidar_link frame\r\n\r\n# Get as ROS message\r\nfrom omni.isaac.ros2_bridge import LaserScanPublisher\r\nlidar_publisher = LaserScanPublisher(\r\n    lidar_prim_path="/World/Robot/lidar_link/lidar",\r\n    topic_name="/scan",\r\n    frame_id="lidar_link"\r\n)\n'})}),"\n",(0,s.jsx)(r.hr,{}),"\n",(0,s.jsx)(r.h2,{id:"imu",children:"IMU Configuration"}),"\n",(0,s.jsx)(r.h3,{id:"creating-imu-sensor",children:"Creating IMU Sensor"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'from omni.isaac.sensor import Imu\r\n\r\n# Create IMU sensor\r\nimu = Imu(\r\n    prim_path="/World/Robot/imu_link/imu",\r\n    translation=np.array([0.0, 0.0, 0.0]),\r\n    orientation=np.array([1.0, 0.0, 0.0, 0.0])\r\n)\r\n\r\nimu.initialize()\n'})}),"\n",(0,s.jsx)(r.h3,{id:"imu-noise-parameters",children:"IMU Noise Parameters"}),"\n",(0,s.jsx)(r.p,{children:"From Module 1 MPU-6050 analysis:"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'# Configure IMU noise to match MEMS specifications\r\nimu_config = {\r\n    # Accelerometer\r\n    "accel_noise_density": 0.0004,  # g/\u221aHz (400 \xb5g/\u221aHz)\r\n    "accel_bias_instability": 0.00004,  # g (40 \xb5g)\r\n\r\n    # Gyroscope\r\n    "gyro_noise_density": 0.005,  # \xb0/s/\u221aHz\r\n    "gyro_bias_instability": 5.0,  # \xb0/hr\r\n\r\n    # Sample rate\r\n    "frequency": 100.0,  # Hz\r\n}\r\n\r\n# At 100 Hz:\r\n# Accel noise = 0.0004 * sqrt(100) * 9.81 = 0.039 m/s\xb2 (matches M3)\r\n# Gyro noise = 0.005 * sqrt(100) * \u03c0/180 = 0.00087 rad/s (matches M3)\n'})}),"\n",(0,s.jsx)(r.h3,{id:"getting-imu-data",children:"Getting IMU Data"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'# Get IMU reading\r\nimu_data = imu.get_current_frame()\r\n\r\n# Returns:\r\n# - linear_acceleration: (3,) array [m/s\xb2]\r\n# - angular_velocity: (3,) array [rad/s]\r\n# - orientation: (4,) quaternion (if available)\r\n\r\n# Publish to ROS\r\nfrom omni.isaac.ros2_bridge import ImuPublisher\r\nimu_publisher = ImuPublisher(\r\n    imu_prim_path="/World/Robot/imu_link/imu",\r\n    topic_name="/imu/data",\r\n    frame_id="imu_link"\r\n)\n'})}),"\n",(0,s.jsx)(r.hr,{}),"\n",(0,s.jsx)(r.h2,{id:"ground-truth",children:"Ground Truth Extraction"}),"\n",(0,s.jsx)(r.p,{children:"Isaac Sim can provide perfect ground truth for validation:"}),"\n",(0,s.jsx)(r.h3,{id:"pose-ground-truth",children:"Pose Ground Truth"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'from omni.isaac.core import World\r\nfrom omni.isaac.core.utils.stage import get_current_stage\r\n\r\n# Get exact robot pose (no noise)\r\nstage = get_current_stage()\r\nrobot_prim = stage.GetPrimAtPath("/World/Robot")\r\n\r\n# Get transform\r\nfrom pxr import UsdGeom\r\nxformable = UsdGeom.Xformable(robot_prim)\r\ntransform = xformable.ComputeLocalToWorldTransform(0)\r\n\r\n# Extract position and orientation\r\n# This is ground truth - use for VSLAM evaluation (Chapter 4)\n'})}),"\n",(0,s.jsx)(r.h3,{id:"sensor-ground-truth",children:"Sensor Ground Truth"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:"# Get perfect depth (no noise)\r\ncamera.add_distance_to_camera_output()\r\nperfect_depth = camera.get_depth_image()  # Before noise is applied\r\n\r\n# Compare to noisy output for validation\n"})}),"\n",(0,s.jsx)(r.h3,{id:"object-annotations",children:"Object Annotations"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:"# Get semantic labels for synthetic data\r\nfrom omni.isaac.synthetic_utils import SyntheticDataHelper\r\n\r\nsd_helper = SyntheticDataHelper()\r\n\r\n# Get instance segmentation\r\ninstance_seg = sd_helper.get_instance_segmentation()\r\n\r\n# Get bounding boxes\r\nbboxes = sd_helper.get_bounding_boxes_2d()\r\n\r\n# Get class labels\r\nlabels = sd_helper.get_semantic_segmentation()\n"})}),"\n",(0,s.jsx)(r.hr,{}),"\n",(0,s.jsx)(r.h2,{id:"complete-example",children:"Complete Sensor Suite Example"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'"""Configure complete sensor suite for warehouse robot."""\r\n\r\nimport numpy as np\r\nfrom omni.isaac.core import World\r\nfrom omni.isaac.sensor import Camera, LidarRtx, Imu\r\nfrom omni.isaac.ros2_bridge import (\r\n    ImagePublisher, LaserScanPublisher, ImuPublisher\r\n)\r\n\r\nclass SensorSuite:\r\n    def __init__(self, robot_path: str):\r\n        self.robot_path = robot_path\r\n\r\n    def setup_sensors(self):\r\n        # RGB-D Camera (RealSense D435)\r\n        self.camera = Camera(\r\n            prim_path=f"{self.robot_path}/camera_link/camera",\r\n            resolution=(640, 480),\r\n            frequency=30\r\n        )\r\n        self.camera.add_distance_to_camera_output()\r\n        self.camera.initialize()\r\n\r\n        # LiDAR (VLP-16)\r\n        self.lidar = LidarRtx(\r\n            prim_path=f"{self.robot_path}/lidar_link/lidar"\r\n        )\r\n        self.lidar.set_fov([360.0, 30.0])\r\n        self.lidar.set_resolution([0.4, 2.0])\r\n        self.lidar.set_valid_range([0.5, 100.0])\r\n        self.lidar.set_rotation_frequency(10.0)\r\n        self.lidar.initialize()\r\n\r\n        # IMU (MPU-6050)\r\n        self.imu = Imu(\r\n            prim_path=f"{self.robot_path}/imu_link/imu"\r\n        )\r\n        self.imu.initialize()\r\n\r\n    def setup_ros_publishers(self):\r\n        # Image publisher\r\n        self.image_pub = ImagePublisher(\r\n            camera_prim_path=f"{self.robot_path}/camera_link/camera",\r\n            topic_name="/camera/image_raw",\r\n            frame_id="camera_link"\r\n        )\r\n\r\n        # Depth publisher\r\n        self.depth_pub = ImagePublisher(\r\n            camera_prim_path=f"{self.robot_path}/camera_link/camera",\r\n            topic_name="/camera/depth/image_raw",\r\n            frame_id="camera_link",\r\n            image_type="depth"\r\n        )\r\n\r\n        # LiDAR publisher\r\n        self.lidar_pub = LaserScanPublisher(\r\n            lidar_prim_path=f"{self.robot_path}/lidar_link/lidar",\r\n            topic_name="/scan",\r\n            frame_id="lidar_link"\r\n        )\r\n\r\n        # IMU publisher\r\n        self.imu_pub = ImuPublisher(\r\n            imu_prim_path=f"{self.robot_path}/imu_link/imu",\r\n            topic_name="/imu/data",\r\n            frame_id="imu_link"\r\n        )\r\n\r\n# Usage\r\nsensor_suite = SensorSuite("/World/Robot")\r\nsensor_suite.setup_sensors()\r\nsensor_suite.setup_ros_publishers()\n'})}),"\n",(0,s.jsx)(r.hr,{}),"\n",(0,s.jsx)(r.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(r.p,{children:"This chapter covered Isaac Sim scene composition and sensor configuration:"}),"\n",(0,s.jsxs)(r.ol,{children:["\n",(0,s.jsxs)(r.li,{children:["\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"USD composition"})," provides layers, references, and variants for modular scenes\u2014more powerful than SDF includes."]}),"\n"]}),"\n",(0,s.jsxs)(r.li,{children:["\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"URDF import"})," brings Module 2 robots into Isaac with articulation physics."]}),"\n"]}),"\n",(0,s.jsxs)(r.li,{children:["\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"RTX cameras"})," use physically-based rendering; configure focal length and aperture to match real camera specs."]}),"\n"]}),"\n",(0,s.jsxs)(r.li,{children:["\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"RTX LiDAR"})," uses GPU ray-tracing for faster, more accurate ranging than Gazebo."]}),"\n"]}),"\n",(0,s.jsxs)(r.li,{children:["\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"IMU sensors"})," configure with noise parameters from Module 1 datasheets."]}),"\n"]}),"\n",(0,s.jsxs)(r.li,{children:["\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"Ground truth"})," enables validation and synthetic data generation."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(r.hr,{}),"\n",(0,s.jsx)(r.h2,{id:"reality-gap-callout",children:"Reality Gap Callout"}),"\n",(0,s.jsx)(r.p,{children:"Even with RTX, gaps remain:"}),"\n",(0,s.jsxs)(r.table,{children:[(0,s.jsx)(r.thead,{children:(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.th,{children:"Sensor"}),(0,s.jsx)(r.th,{children:"RTX Improvement"}),(0,s.jsx)(r.th,{children:"Remaining Gap"})]})}),(0,s.jsxs)(r.tbody,{children:[(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"Camera"}),(0,s.jsx)(r.td,{children:"Photorealistic materials"}),(0,s.jsx)(r.td,{children:"Some lighting, no lens dust"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"LiDAR"}),(0,s.jsx)(r.td,{children:"Better ray-tracing"}),(0,s.jsx)(r.td,{children:"Multi-path simplified"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"IMU"}),(0,s.jsx)(r.td,{children:"Configurable noise"}),(0,s.jsx)(r.td,{children:"Non-Gaussian outliers"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"Depth"}),(0,s.jsx)(r.td,{children:"Better accuracy"}),(0,s.jsx)(r.td,{children:"Edge artifacts differ"})]})]})]}),"\n",(0,s.jsx)(r.p,{children:"Use domain randomization (Chapter 5) to address remaining gaps."}),"\n",(0,s.jsx)(r.hr,{}),"\n",(0,s.jsx)(r.h2,{id:"self-assessment-questions",children:"Self-Assessment Questions"}),"\n",(0,s.jsxs)(r.ol,{children:["\n",(0,s.jsxs)(r.li,{children:["\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"USD Composition"}),": You have a warehouse.usd and three different robot.usd files. How would you create a scene that can switch between robots without duplicating the warehouse?"]}),"\n"]}),"\n",(0,s.jsxs)(r.li,{children:["\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"URDF Import"}),": Your imported robot floats 10cm above the ground. What import setting likely caused this?"]}),"\n"]}),"\n",(0,s.jsxs)(r.li,{children:["\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"Camera Matching"}),": Your real camera has 90\xb0 HFOV. The Isaac camera shows 60\xb0. What parameters need adjustment?"]}),"\n"]}),"\n",(0,s.jsxs)(r.li,{children:["\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"LiDAR Comparison"}),": Compare the LiDAR configuration here to your Module 3 Gazebo config. What's different?"]}),"\n"]}),"\n",(0,s.jsxs)(r.li,{children:["\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"Ground Truth Use"}),": You're evaluating a depth estimation neural network. How would you use Isaac's ground truth depth for validation?"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(r.hr,{}),"\n",(0,s.jsx)(r.h2,{id:"whats-next",children:"What's Next"}),"\n",(0,s.jsxs)(r.p,{children:["In ",(0,s.jsx)(r.a,{href:"/module-4/chapter-3-isaac-ros-integration",children:"Chapter 3: Isaac ROS Integration"}),", you'll connect Isaac Sim to ROS 2 using Isaac ROS packages for GPU-accelerated perception pipelines."]})]})}function h(e={}){const{wrapper:r}={...(0,a.R)(),...e.components};return r?(0,s.jsx)(r,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453(e,r,n){n.d(r,{R:()=>t,x:()=>o});var i=n(6540);const s={},a=i.createContext(s);function t(e){const r=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function o(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),i.createElement(a.Provider,{value:r},e.children)}}}]);