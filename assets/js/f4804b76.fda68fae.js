"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[8564],{7795(e,n,r){r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-4/chapter-3-isaac-ros-integration","title":"Chapter 3: Isaac ROS Integration","description":"Chapter Goal","source":"@site/docs/module-4/chapter-3-isaac-ros-integration.md","sourceDirName":"module-4","slug":"/module-4/chapter-3-isaac-ros-integration","permalink":"/Master-Robotics-AI/textbook/module-4/chapter-3-isaac-ros-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/HasnainCodeHub/Master-Robotics-AI/tree/main/docs/docs/module-4/chapter-3-isaac-ros-integration.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"id":"chapter-3-isaac-ros-integration","title":"Chapter 3: Isaac ROS Integration","sidebar_label":"3. Isaac ROS","sidebar_position":4},"sidebar":"textbookSidebar","previous":{"title":"2. Scenes & Sensors","permalink":"/Master-Robotics-AI/textbook/module-4/chapter-2-scene-composition-sensors"},"next":{"title":"4. Visual SLAM","permalink":"/Master-Robotics-AI/textbook/module-4/chapter-4-visual-slam"}}');var s=r(4848),t=r(8453);const a={id:"chapter-3-isaac-ros-integration",title:"Chapter 3: Isaac ROS Integration",sidebar_label:"3. Isaac ROS",sidebar_position:4},o="Chapter 3: Isaac ROS Integration",c={},d=[{value:"Chapter Goal",id:"chapter-goal",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Isaac ROS Ecosystem Overview",id:"isaac-ros-ecosystem-overview",level:2},{value:"NITROS: Zero-Copy GPU Messaging",id:"nitros",level:2},{value:"The Problem",id:"the-problem",level:3},{value:"NITROS Solution",id:"nitros-solution",level:3},{value:"Using NITROS",id:"using-nitros",level:3},{value:"When NITROS Matters",id:"when-nitros-matters",level:3},{value:"Isaac ROS Bridge Configuration",id:"bridge",level:2},{value:"Enabling ROS 2 Bridge in Isaac Sim",id:"enabling-ros-2-bridge-in-isaac-sim",level:3},{value:"Launch Configuration",id:"launch-configuration",level:3},{value:"Verifying Bridge",id:"verifying-bridge",level:3},{value:"Isaac ROS Image Pipeline",id:"image-pipeline",level:2},{value:"GPU-Accelerated Image Processing",id:"gpu-accelerated-image-processing",level:3},{value:"Performance Comparison",id:"performance-comparison",level:3},{value:"Isaac ROS Object Detection",id:"detection",level:2},{value:"Using Pre-trained Models",id:"using-pre-trained-models",level:3},{value:"Detection Output",id:"detection-output",level:3},{value:"Building Perception Nodes",id:"perception-nodes",level:2},{value:"Complete Perception Pipeline",id:"complete-perception-pipeline",level:3},{value:"End-to-End Latency Analysis",id:"latency",level:2},{value:"Perception Pipeline Timing",id:"perception-pipeline-timing",level:3},{value:"Comparison",id:"comparison",level:3},{value:"Summary",id:"summary",level:2},{value:"Self-Assessment Questions",id:"self-assessment-questions",level:2},{value:"What&#39;s Next",id:"whats-next",level:2}];function l(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-3-isaac-ros-integration",children:"Chapter 3: Isaac ROS Integration"})}),"\n",(0,s.jsx)(n.h2,{id:"chapter-goal",children:"Chapter Goal"}),"\n",(0,s.jsxs)(n.p,{children:["By the end of this chapter, you will be able to ",(0,s.jsx)(n.strong,{children:"integrate Isaac Sim with ROS 2 using Isaac ROS packages"}),", enabling GPU-accelerated perception pipelines that process simulated or real sensor data."]}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"ID"}),(0,s.jsx)(n.th,{children:"Objective"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"3.1"}),(0,s.jsx)(n.td,{children:"Configure Isaac ROS bridge to connect Isaac Sim to ROS 2 topics"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"3.2"}),(0,s.jsx)(n.td,{children:"Explain NITROS and zero-copy GPU messaging"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"3.3"}),(0,s.jsx)(n.td,{children:"Configure Isaac ROS Image Pipeline for GPU-accelerated processing"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"3.4"}),(0,s.jsx)(n.td,{children:"Configure Isaac ROS Object Detection for real-time inference"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"3.5"}),(0,s.jsx)(n.td,{children:"Implement perception nodes using Isaac ROS with standard ROS 2 patterns"})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"isaac-ros-ecosystem-overview",children:"Isaac ROS Ecosystem Overview"}),"\n",(0,s.jsx)(n.p,{children:"Isaac ROS provides GPU-accelerated ROS 2 packages:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                    Isaac ROS Architecture                        \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502                                                                  \u2502\r\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     NITROS     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\r\n\u2502  \u2502  Isaac Sim   \u2502 \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba \u2502    Isaac ROS Packages    \u2502   \u2502\r\n\u2502  \u2502  (Sensors)   \u2502   Zero-copy    \u2502                          \u2502   \u2502\r\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       GPU      \u2502  - Image Pipeline        \u2502   \u2502\r\n\u2502         \u2502                        \u2502  - Object Detection      \u2502   \u2502\r\n\u2502         \u2502                        \u2502  - Visual SLAM           \u2502   \u2502\r\n\u2502         \u25bc                        \u2502  - Depth Estimation      \u2502   \u2502\r\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502  - Segmentation          \u2502   \u2502\r\n\u2502  \u2502  Hardware    \u2502                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\r\n\u2502  \u2502  (Real Robot)\u2502                          \u2502                     \u2502\r\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                          \u2502                     \u2502\r\n\u2502                                            \u25bc                     \u2502\r\n\u2502                               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\r\n\u2502                               \u2502  Standard ROS 2 Nodes    \u2502       \u2502\r\n\u2502                               \u2502  (Your Application)      \u2502       \u2502\r\n\u2502                               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\r\n\u2502                                                                  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Key Point"}),": The same Isaac ROS packages work with both Isaac Sim and real hardware."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"nitros",children:"NITROS: Zero-Copy GPU Messaging"}),"\n",(0,s.jsx)(n.h3,{id:"the-problem",children:"The Problem"}),"\n",(0,s.jsx)(n.p,{children:"Standard ROS 2 messaging copies data:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"GPU (Sensor) \u2192 CPU \u2192 ROS 2 Message \u2192 CPU \u2192 GPU (Processing)\r\n     \u2502                   \u2502                        \u2502\r\n     \u2514\u2500\u2500 Copy #1 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                        \u2502\r\n                         \u2514\u2500\u2500 Copy #2 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.p,{children:"For HD images at 30 fps, each copy takes milliseconds\u2014unacceptable for real-time."}),"\n",(0,s.jsx)(n.h3,{id:"nitros-solution",children:"NITROS Solution"}),"\n",(0,s.jsx)(n.p,{children:"NITROS (NVIDIA Isaac Transport for ROS) keeps data on GPU:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"GPU (Sensor) \u2192 NITROS Message \u2192 GPU (Processing)\r\n                    \u2502\r\n                    \u2514\u2500\u2500 Zero copies, GPU memory stays on GPU\n"})}),"\n",(0,s.jsx)(n.h3,{id:"using-nitros",children:"Using NITROS"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Standard ROS 2 subscriber receives CPU data\r\nfrom sensor_msgs.msg import Image\r\n# ... receive Image, data on CPU\r\n\r\n# NITROS-enabled subscriber receives GPU data\r\nfrom isaac_ros_nitros_bridge_interfaces.msg import NitrosBridgeTensorList\r\n# ... receive tensor on GPU, no copy needed\n"})}),"\n",(0,s.jsx)(n.h3,{id:"when-nitros-matters",children:"When NITROS Matters"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Pipeline"}),(0,s.jsx)(n.th,{children:"Without NITROS"}),(0,s.jsx)(n.th,{children:"With NITROS"}),(0,s.jsx)(n.th,{children:"Speedup"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"HD image rectification"}),(0,s.jsx)(n.td,{children:"15ms"}),(0,s.jsx)(n.td,{children:"3ms"}),(0,s.jsx)(n.td,{children:"5x"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Object detection"}),(0,s.jsx)(n.td,{children:"25ms"}),(0,s.jsx)(n.td,{children:"8ms"}),(0,s.jsx)(n.td,{children:"3x"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Full perception pipeline"}),(0,s.jsx)(n.td,{children:"50ms"}),(0,s.jsx)(n.td,{children:"12ms"}),(0,s.jsx)(n.td,{children:"4x"})]})]})]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Physical Grounding"}),": At 30 fps, each frame has 33ms budget. Without NITROS, perception alone exceeds this. With NITROS, you have margin for control."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"bridge",children:"Isaac ROS Bridge Configuration"}),"\n",(0,s.jsx)(n.h3,{id:"enabling-ros-2-bridge-in-isaac-sim",children:"Enabling ROS 2 Bridge in Isaac Sim"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# In Isaac Sim Python script\r\nfrom omni.isaac.ros2_bridge import enable_ros2_bridge\r\n\r\n# Enable the bridge\r\nenable_ros2_bridge()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"launch-configuration",children:"Launch Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# launch/isaac_sim_bridge.launch.py\r\nfrom launch import LaunchDescription\r\nfrom launch.actions import ExecuteProcess\r\nfrom launch_ros.actions import Node\r\n\r\ndef generate_launch_description():\r\n    return LaunchDescription([\r\n        # Isaac Sim ROS 2 bridge is enabled in the sim\r\n        # Configure topic mappings\r\n\r\n        # Image topic bridge\r\n        Node(\r\n            package='isaac_ros_nitros_bridge_ros2',\r\n            executable='image_converter_node',\r\n            name='image_bridge',\r\n            parameters=[{\r\n                'input_topic': '/isaac_sim/camera/image',\r\n                'output_topic': '/camera/image_raw',\r\n            }]\r\n        ),\r\n    ])\n"})}),"\n",(0,s.jsx)(n.h3,{id:"verifying-bridge",children:"Verifying Bridge"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Isaac Sim running with ROS bridge\r\n\r\n# Terminal 2: Check topics\r\nros2 topic list\r\n\r\n# Expected topics:\r\n# /camera/image_raw\r\n# /camera/camera_info\r\n# /scan\r\n# /imu/data\r\n# /odom\r\n# /cmd_vel\r\n\r\n# Check data flow\r\nros2 topic hz /camera/image_raw\r\n# Should show ~30 Hz if camera is 30 fps\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"image-pipeline",children:"Isaac ROS Image Pipeline"}),"\n",(0,s.jsx)(n.h3,{id:"gpu-accelerated-image-processing",children:"GPU-Accelerated Image Processing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# launch/image_pipeline.launch.py\r\nfrom launch import LaunchDescription\r\nfrom launch_ros.actions import ComposableNodeContainer\r\nfrom launch_ros.descriptions import ComposableNode\r\n\r\ndef generate_launch_description():\r\n    return LaunchDescription([\r\n        ComposableNodeContainer(\r\n            name='image_pipeline_container',\r\n            namespace='',\r\n            package='rclcpp_components',\r\n            executable='component_container_mt',\r\n            composable_node_descriptions=[\r\n                # Rectification\r\n                ComposableNode(\r\n                    package='isaac_ros_image_proc',\r\n                    plugin='nvidia::isaac_ros::image_proc::RectifyNode',\r\n                    name='rectify_node',\r\n                    remappings=[\r\n                        ('image_raw', '/camera/image_raw'),\r\n                        ('camera_info', '/camera/camera_info'),\r\n                        ('image_rect', '/camera/image_rect'),\r\n                    ]\r\n                ),\r\n                # Color conversion\r\n                ComposableNode(\r\n                    package='isaac_ros_image_proc',\r\n                    plugin='nvidia::isaac_ros::image_proc::ImageFormatConverterNode',\r\n                    name='format_converter',\r\n                    parameters=[{\r\n                        'encoding_desired': 'rgb8',\r\n                    }],\r\n                    remappings=[\r\n                        ('image_raw', '/camera/image_rect'),\r\n                        ('image', '/camera/image_rgb'),\r\n                    ]\r\n                ),\r\n                # Resize for inference\r\n                ComposableNode(\r\n                    package='isaac_ros_image_proc',\r\n                    plugin='nvidia::isaac_ros::image_proc::ResizeNode',\r\n                    name='resize_node',\r\n                    parameters=[{\r\n                        'output_width': 640,\r\n                        'output_height': 480,\r\n                    }],\r\n                    remappings=[\r\n                        ('image', '/camera/image_rgb'),\r\n                        ('camera_info', '/camera/camera_info'),\r\n                        ('resize/image', '/camera/image_resized'),\r\n                        ('resize/camera_info', '/camera/camera_info_resized'),\r\n                    ]\r\n                ),\r\n            ],\r\n            output='screen',\r\n        ),\r\n    ])\n"})}),"\n",(0,s.jsx)(n.h3,{id:"performance-comparison",children:"Performance Comparison"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Operation"}),(0,s.jsx)(n.th,{children:"CPU (ROS 2)"}),(0,s.jsx)(n.th,{children:"GPU (Isaac ROS)"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Rectification"}),(0,s.jsx)(n.td,{children:"8ms"}),(0,s.jsx)(n.td,{children:"1ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Color conversion"}),(0,s.jsx)(n.td,{children:"3ms"}),(0,s.jsx)(n.td,{children:"0.5ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Resize"}),(0,s.jsx)(n.td,{children:"5ms"}),(0,s.jsx)(n.td,{children:"0.3ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Total"})}),(0,s.jsx)(n.td,{children:"16ms"}),(0,s.jsx)(n.td,{children:"1.8ms"})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"detection",children:"Isaac ROS Object Detection"}),"\n",(0,s.jsx)(n.h3,{id:"using-pre-trained-models",children:"Using Pre-trained Models"}),"\n",(0,s.jsx)(n.p,{children:"Isaac ROS includes optimized detection models:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# launch/object_detection.launch.py\r\nfrom launch import LaunchDescription\r\nfrom launch_ros.actions import ComposableNodeContainer\r\nfrom launch_ros.descriptions import ComposableNode\r\n\r\ndef generate_launch_description():\r\n    return LaunchDescription([\r\n        ComposableNodeContainer(\r\n            name='detection_container',\r\n            namespace='',\r\n            package='rclcpp_components',\r\n            executable='component_container_mt',\r\n            composable_node_descriptions=[\r\n                # DNN Image Encoder\r\n                ComposableNode(\r\n                    package='isaac_ros_dnn_image_encoder',\r\n                    plugin='nvidia::isaac_ros::dnn_inference::DnnImageEncoderNode',\r\n                    name='dnn_encoder',\r\n                    parameters=[{\r\n                        'input_image_width': 640,\r\n                        'input_image_height': 480,\r\n                        'network_image_width': 640,\r\n                        'network_image_height': 480,\r\n                        'image_mean': [0.5, 0.5, 0.5],\r\n                        'image_stddev': [0.5, 0.5, 0.5],\r\n                    }],\r\n                    remappings=[\r\n                        ('image', '/camera/image_resized'),\r\n                        ('encoded_tensor', '/tensor_pub'),\r\n                    ]\r\n                ),\r\n                # TensorRT Inference\r\n                ComposableNode(\r\n                    package='isaac_ros_tensor_rt',\r\n                    plugin='nvidia::isaac_ros::dnn_inference::TensorRTNode',\r\n                    name='tensor_rt',\r\n                    parameters=[{\r\n                        'model_file_path': '/models/yolov8.onnx',\r\n                        'engine_file_path': '/models/yolov8.engine',\r\n                        'input_tensor_names': ['images'],\r\n                        'input_binding_names': ['images'],\r\n                        'output_tensor_names': ['output0'],\r\n                        'output_binding_names': ['output0'],\r\n                    }],\r\n                ),\r\n                # Detection Decoder\r\n                ComposableNode(\r\n                    package='isaac_ros_detectnet',\r\n                    plugin='nvidia::isaac_ros::detectnet::DetectNetDecoderNode',\r\n                    name='detectnet_decoder',\r\n                    parameters=[{\r\n                        'label_list': ['person', 'box', 'shelf'],\r\n                        'confidence_threshold': 0.5,\r\n                    }],\r\n                ),\r\n            ],\r\n            output='screen',\r\n        ),\r\n    ])\n"})}),"\n",(0,s.jsx)(n.h3,{id:"detection-output",children:"Detection Output"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Subscribing to detections\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom vision_msgs.msg import Detection2DArray\r\n\r\nclass DetectionSubscriber(Node):\r\n    def __init__(self):\r\n        super().__init__('detection_subscriber')\r\n\r\n        self.subscription = self.create_subscription(\r\n            Detection2DArray,\r\n            '/detectnet/detections',\r\n            self.detection_callback,\r\n            10\r\n        )\r\n\r\n    def detection_callback(self, msg):\r\n        for detection in msg.detections:\r\n            # Bounding box\r\n            bbox = detection.bbox\r\n            center_x = bbox.center.position.x\r\n            center_y = bbox.center.position.y\r\n            width = bbox.size_x\r\n            height = bbox.size_y\r\n\r\n            # Class and confidence\r\n            for result in detection.results:\r\n                class_id = result.hypothesis.class_id\r\n                confidence = result.hypothesis.score\r\n\r\n                self.get_logger().info(\r\n                    f'Detected {class_id} at ({center_x}, {center_y}) '\r\n                    f'with confidence {confidence:.2f}'\r\n                )\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"perception-nodes",children:"Building Perception Nodes"}),"\n",(0,s.jsx)(n.h3,{id:"complete-perception-pipeline",children:"Complete Perception Pipeline"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nPerception node using Isaac ROS packages.\r\nSubscribes to camera, runs detection, publishes results.\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom rclpy.qos import QoSProfile, ReliabilityPolicy\r\nfrom sensor_msgs.msg import Image\r\nfrom vision_msgs.msg import Detection2DArray\r\nfrom geometry_msgs.msg import PointStamped\r\n\r\n\r\nclass PerceptionNode(Node):\r\n    def __init__(self):\r\n        super().__init__('perception_node')\r\n\r\n        # QoS for sensor data\r\n        sensor_qos = QoSProfile(\r\n            reliability=ReliabilityPolicy.BEST_EFFORT,\r\n            depth=1\r\n        )\r\n\r\n        # Subscribe to Isaac ROS detection output\r\n        self.detection_sub = self.create_subscription(\r\n            Detection2DArray,\r\n            '/detectnet/detections',\r\n            self.detection_callback,\r\n            10\r\n        )\r\n\r\n        # Subscribe to depth for 3D localization\r\n        self.depth_sub = self.create_subscription(\r\n            Image,\r\n            '/camera/depth/image_raw',\r\n            self.depth_callback,\r\n            sensor_qos\r\n        )\r\n\r\n        # Publish target locations\r\n        self.target_pub = self.create_publisher(\r\n            PointStamped,\r\n            '/perception/target',\r\n            10\r\n        )\r\n\r\n        self.latest_depth = None\r\n\r\n        self.get_logger().info('Perception node started')\r\n\r\n    def depth_callback(self, msg):\r\n        self.latest_depth = msg\r\n\r\n    def detection_callback(self, msg):\r\n        if self.latest_depth is None:\r\n            return\r\n\r\n        for detection in msg.detections:\r\n            # Get detection center\r\n            cx = int(detection.bbox.center.position.x)\r\n            cy = int(detection.bbox.center.position.y)\r\n\r\n            # Get depth at detection center\r\n            depth = self.get_depth_at_pixel(cx, cy)\r\n\r\n            if depth is None or depth <= 0:\r\n                continue\r\n\r\n            # Convert to 3D point (simplified pinhole model)\r\n            fx = 554.25  # From camera intrinsics\r\n            fy = 554.25\r\n            cx_cam = 320  # Principal point\r\n            cy_cam = 240\r\n\r\n            x = (cx - cx_cam) * depth / fx\r\n            y = (cy - cy_cam) * depth / fy\r\n            z = depth\r\n\r\n            # Publish target\r\n            target = PointStamped()\r\n            target.header.stamp = self.get_clock().now().to_msg()\r\n            target.header.frame_id = 'camera_link'\r\n            target.point.x = float(z)   # Camera Z \u2192 forward\r\n            target.point.y = float(-x)  # Camera X \u2192 right\r\n            target.point.z = float(-y)  # Camera Y \u2192 down\r\n\r\n            self.target_pub.publish(target)\r\n\r\n            self.get_logger().info(\r\n                f'Target at ({x:.2f}, {y:.2f}, {z:.2f})'\r\n            )\r\n\r\n    def get_depth_at_pixel(self, x, y):\r\n        \"\"\"Extract depth value at pixel coordinates.\"\"\"\r\n        if self.latest_depth is None:\r\n            return None\r\n\r\n        # Depth image format: 32FC1 or 16UC1\r\n        import numpy as np\r\n        import struct\r\n\r\n        width = self.latest_depth.width\r\n        height = self.latest_depth.height\r\n\r\n        if x < 0 or x >= width or y < 0 or y >= height:\r\n            return None\r\n\r\n        if self.latest_depth.encoding == '32FC1':\r\n            # 32-bit float\r\n            idx = (y * width + x) * 4\r\n            depth = struct.unpack('f', self.latest_depth.data[idx:idx+4])[0]\r\n        elif self.latest_depth.encoding == '16UC1':\r\n            # 16-bit unsigned (millimeters typically)\r\n            idx = (y * width + x) * 2\r\n            depth = struct.unpack('H', self.latest_depth.data[idx:idx+2])[0]\r\n            depth = depth / 1000.0  # Convert to meters\r\n        else:\r\n            return None\r\n\r\n        return depth\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = PerceptionNode()\r\n    rclpy.spin(node)\r\n    node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"latency",children:"End-to-End Latency Analysis"}),"\n",(0,s.jsx)(n.h3,{id:"perception-pipeline-timing",children:"Perception Pipeline Timing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502              End-to-End Perception Latency                       \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502                                                                  \u2502\r\n\u2502  Isaac Sim Camera \u2500\u2500\u25ba NITROS Bridge \u2500\u2500\u25ba Image Pipeline          \u2502\r\n\u2502       \u2502                    \u2502                  \u2502                  \u2502\r\n\u2502       \u2514\u2500\u2500 Render: 5ms      \u2514\u2500\u2500 0ms           \u2514\u2500\u2500 2ms            \u2502\r\n\u2502                                                                  \u2502\r\n\u2502  \u2500\u2500\u25ba Detection \u2500\u2500\u25ba Decoder \u2500\u2500\u25ba Perception Node \u2500\u2500\u25ba Output       \u2502\r\n\u2502          \u2502            \u2502              \u2502                \u2502          \u2502\r\n\u2502          \u2514\u2500\u2500 8ms      \u2514\u2500\u2500 1ms       \u2514\u2500\u2500 2ms         Total: 18ms \u2502\r\n\u2502                                                                  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.h3,{id:"comparison",children:"Comparison"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Configuration"}),(0,s.jsx)(n.th,{children:"Latency"}),(0,s.jsx)(n.th,{children:"Real-time at 30fps?"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"CPU-only pipeline"}),(0,s.jsx)(n.td,{children:"60ms"}),(0,s.jsx)(n.td,{children:"No (budget: 33ms)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"GPU without NITROS"}),(0,s.jsx)(n.td,{children:"35ms"}),(0,s.jsx)(n.td,{children:"Marginal"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Isaac ROS with NITROS"}),(0,s.jsx)(n.td,{children:"18ms"}),(0,s.jsx)(n.td,{children:"Yes"})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"This chapter covered Isaac ROS integration:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS packages"})," provide GPU-accelerated perception for both simulation and hardware."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"NITROS"})," eliminates CPU-GPU data copies, critical for real-time performance."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS bridge"})," connects Isaac Sim sensors to ROS 2 topics."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Image pipeline"})," provides GPU-accelerated rectification, conversion, and resize."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Object detection"})," uses TensorRT for optimized inference on camera streams."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Perception nodes"})," combine Isaac ROS outputs into robot decisions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"self-assessment-questions",children:"Self-Assessment Questions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"NITROS Benefit"}),": Your perception pipeline without NITROS takes 45ms per frame. Your control loop needs 30 fps. What's the problem and how does NITROS help?"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Bridge Verification"}),": You enabled the ROS 2 bridge but ",(0,s.jsx)(n.code,{children:"ros2 topic list"})," shows no Isaac topics. What would you check?"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Detection Pipeline"}),": The detection node outputs bounding boxes but your perception node needs 3D positions. What additional sensor data do you need?"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Latency Budget"}),": Your robot needs to detect and avoid obstacles at 1 m/s with 0.5m stopping distance. What's your maximum perception latency?"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Same Code, Different Data"}),": How do you switch your perception node from Isaac Sim camera to real camera without code changes?"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"whats-next",children:"What's Next"}),"\n",(0,s.jsxs)(n.p,{children:["In ",(0,s.jsx)(n.a,{href:"/module-4/chapter-4-visual-slam",children:"Chapter 4: Visual SLAM"}),", you'll implement Isaac ROS Visual SLAM for robust robot localization, enabling navigation and manipulation tasks."]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},8453(e,n,r){r.d(n,{R:()=>a,x:()=>o});var i=r(6540);const s={},t=i.createContext(s);function a(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);