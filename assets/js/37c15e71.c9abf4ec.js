"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[8651],{2460(e,n,r){r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>d,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-5/chapter-1-speech-recognition","title":"Chapter 1: Speech Recognition","description":"Chapter Goal","source":"@site/docs/module-5/chapter-1-speech-recognition.md","sourceDirName":"module-5","slug":"/module-5/chapter-1-speech-recognition","permalink":"/Master-Robotics-AI/textbook/module-5/chapter-1-speech-recognition","draft":false,"unlisted":false,"editUrl":"https://github.com/HasnainCodeHub/Master-Robotics-AI/tree/main/docs/docs/module-5/chapter-1-speech-recognition.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"id":"chapter-1-speech-recognition","title":"Chapter 1: Speech Recognition","sidebar_label":"1. Speech Recognition","sidebar_position":2},"sidebar":"textbookSidebar","previous":{"title":"Module 5 Overview","permalink":"/Master-Robotics-AI/textbook/module-5/"},"next":{"title":"2. LLM Task Planning","permalink":"/Master-Robotics-AI/textbook/module-5/chapter-2-llm-task-planning"}}');var i=r(4848),t=r(8453);const o={id:"chapter-1-speech-recognition",title:"Chapter 1: Speech Recognition",sidebar_label:"1. Speech Recognition",sidebar_position:2},d="Chapter 1: Speech Recognition for Robot Command",l={},c=[{value:"Chapter Goal",id:"chapter-goal",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Why Voice Commands for Robots?",id:"why-voice-commands-for-robots",level:2},{value:"Whisper Architecture Overview",id:"architecture",level:2},{value:"Processing Pipeline",id:"processing-pipeline",level:3},{value:"Key Components",id:"key-components",level:3},{value:"Model Sizes",id:"model-sizes",level:3},{value:"Deployment",id:"deployment",level:2},{value:"Local Inference with faster-whisper",id:"local-inference-with-faster-whisper",level:3},{value:"Latency Measurements",id:"latency-measurements",level:3},{value:"Audio Capture Pipeline",id:"audio-pipeline",level:2},{value:"ROS 2 Audio Capture Node",id:"ros-2-audio-capture-node",level:3},{value:"Voice Activity Detection (VAD)",id:"voice-activity-detection-vad",level:3},{value:"Prompt Conditioning",id:"conditioning",level:2},{value:"Domain-Specific Recognition",id:"domain-specific-recognition",level:3},{value:"Vocabulary Boosting",id:"vocabulary-boosting",level:3},{value:"Noise Robustness",id:"noise",level:2},{value:"Robot Environmental Noise",id:"robot-environmental-noise",level:3},{value:"Mitigation Strategies",id:"mitigation-strategies",level:3},{value:"Noise Robustness Testing",id:"noise-robustness-testing",level:3},{value:"Summary",id:"summary",level:2},{value:"Self-Assessment Questions",id:"self-assessment-questions",level:2},{value:"What&#39;s Next",id:"whats-next",level:2}];function a(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-1-speech-recognition-for-robot-command",children:"Chapter 1: Speech Recognition for Robot Command"})}),"\n",(0,i.jsx)(n.h2,{id:"chapter-goal",children:"Chapter Goal"}),"\n",(0,i.jsxs)(n.p,{children:["By the end of this chapter, you will be able to ",(0,i.jsx)(n.strong,{children:"integrate Whisper for voice command input"}),", understanding its architecture, deployment options, latency characteristics, and robustness to acoustic noise in robotic environments."]}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"ID"}),(0,i.jsx)(n.th,{children:"Objective"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"1.1"}),(0,i.jsx)(n.td,{children:"Explain Whisper's architecture conceptually"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"1.2"}),(0,i.jsx)(n.td,{children:"Deploy Whisper with appropriate model size selection"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"1.3"}),(0,i.jsx)(n.td,{children:"Implement an audio capture pipeline with voice activity detection"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"1.4"}),(0,i.jsx)(n.td,{children:"Configure Whisper for robot command recognition"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"1.5"}),(0,i.jsx)(n.td,{children:"Evaluate robustness to environmental noise"})]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"why-voice-commands-for-robots",children:"Why Voice Commands for Robots?"}),"\n",(0,i.jsx)(n.p,{children:"Voice is natural for humans but challenging for robots:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Input Method"}),(0,i.jsx)(n.th,{children:"Pros"}),(0,i.jsx)(n.th,{children:"Cons"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Keyboard/GUI"}),(0,i.jsx)(n.td,{children:"Precise, reliable"}),(0,i.jsx)(n.td,{children:"Requires hands, screen"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Joystick"}),(0,i.jsx)(n.td,{children:"Real-time control"}),(0,i.jsx)(n.td,{children:"Limited vocabulary"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Voice"})}),(0,i.jsx)(n.td,{children:"Hands-free, natural"}),(0,i.jsx)(n.td,{children:"Noise, ambiguity, latency"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:'Voice enables commands like "pick up the red mug" that would require complex menus otherwise.'}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"architecture",children:"Whisper Architecture Overview"}),"\n",(0,i.jsx)(n.p,{children:"Whisper is OpenAI's speech-to-text transformer model."}),"\n",(0,i.jsx)(n.h3,{id:"processing-pipeline",children:"Processing Pipeline"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Audio Waveform \u2500\u2500\u25ba Mel Spectrogram \u2500\u2500\u25ba Encoder \u2500\u2500\u25ba Decoder \u2500\u2500\u25ba Text\r\n      \u2502                  \u2502                \u2502            \u2502\r\n   16kHz            Log-mel         Transformer    Autoregressive\r\n   mono             features         layers         token generation\n"})}),"\n",(0,i.jsx)(n.h3,{id:"key-components",children:"Key Components"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Mel Spectrogram"}),": Converts audio to frequency-time representation matching human hearing."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Encoder"}),": Processes the spectrogram, outputs context vectors."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Decoder"}),": Generates text tokens autoregressively."]}),"\n",(0,i.jsx)(n.h3,{id:"model-sizes",children:"Model Sizes"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{children:"Parameters"}),(0,i.jsx)(n.th,{children:"VRAM"}),(0,i.jsx)(n.th,{children:"Speed"}),(0,i.jsx)(n.th,{children:"WER"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"tiny"}),(0,i.jsx)(n.td,{children:"39M"}),(0,i.jsx)(n.td,{children:"~1 GB"}),(0,i.jsx)(n.td,{children:"Fastest"}),(0,i.jsx)(n.td,{children:"Higher"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"base"}),(0,i.jsx)(n.td,{children:"74M"}),(0,i.jsx)(n.td,{children:"~1 GB"}),(0,i.jsx)(n.td,{children:"Fast"}),(0,i.jsx)(n.td,{children:"Good"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"small"}),(0,i.jsx)(n.td,{children:"244M"}),(0,i.jsx)(n.td,{children:"~2 GB"}),(0,i.jsx)(n.td,{children:"Moderate"}),(0,i.jsx)(n.td,{children:"Better"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"medium"}),(0,i.jsx)(n.td,{children:"769M"}),(0,i.jsx)(n.td,{children:"~5 GB"}),(0,i.jsx)(n.td,{children:"Slower"}),(0,i.jsx)(n.td,{children:"Best"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"large"}),(0,i.jsx)(n.td,{children:"1550M"}),(0,i.jsx)(n.td,{children:"~10 GB"}),(0,i.jsx)(n.td,{children:"Slowest"}),(0,i.jsx)(n.td,{children:"Best"})]})]})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Recommendation"}),": Start with ",(0,i.jsx)(n.code,{children:"base"})," for real-time robot commands; use ",(0,i.jsx)(n.code,{children:"small"})," if accuracy insufficient."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"deployment",children:"Deployment"}),"\n",(0,i.jsx)(n.h3,{id:"local-inference-with-faster-whisper",children:"Local Inference with faster-whisper"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""Whisper deployment for robot commands."""\r\n\r\nfrom faster_whisper import WhisperModel\r\nimport numpy as np\r\nimport time\r\n\r\nclass WhisperASR:\r\n    def __init__(self, model_size: str = "base"):\r\n        # Use faster-whisper for optimized inference\r\n        self.model = WhisperModel(\r\n            model_size,\r\n            device="cuda",\r\n            compute_type="float16"  # Half precision for speed\r\n        )\r\n        self.sample_rate = 16000\r\n\r\n    def transcribe(self, audio: np.ndarray) -> tuple:\r\n        """\r\n        Transcribe audio to text.\r\n\r\n        Args:\r\n            audio: Float32 numpy array of audio samples at 16kHz\r\n\r\n        Returns:\r\n            (text, latency_ms)\r\n        """\r\n        start = time.time()\r\n\r\n        segments, info = self.model.transcribe(\r\n            audio,\r\n            language="en",\r\n            task="transcribe",\r\n            beam_size=1,  # Greedy for speed\r\n            vad_filter=True,  # Filter silence\r\n        )\r\n\r\n        text = " ".join([seg.text for seg in segments])\r\n        latency_ms = (time.time() - start) * 1000\r\n\r\n        return text.strip(), latency_ms\r\n\r\n# Usage\r\nasr = WhisperASR(model_size="base")\r\ntext, latency = asr.transcribe(audio_samples)\r\nprint(f"Transcribed: \'{text}\' in {latency:.0f}ms")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"latency-measurements",children:"Latency Measurements"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{children:"2s Audio"}),(0,i.jsx)(n.th,{children:"5s Audio"}),(0,i.jsx)(n.th,{children:"10s Audio"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"tiny"}),(0,i.jsx)(n.td,{children:"50ms"}),(0,i.jsx)(n.td,{children:"80ms"}),(0,i.jsx)(n.td,{children:"150ms"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"base"}),(0,i.jsx)(n.td,{children:"100ms"}),(0,i.jsx)(n.td,{children:"200ms"}),(0,i.jsx)(n.td,{children:"400ms"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"small"}),(0,i.jsx)(n.td,{children:"250ms"}),(0,i.jsx)(n.td,{children:"500ms"}),(0,i.jsx)(n.td,{children:"1000ms"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"medium"}),(0,i.jsx)(n.td,{children:"600ms"}),(0,i.jsx)(n.td,{children:"1200ms"}),(0,i.jsx)(n.td,{children:"2500ms"})]})]})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Physical Grounding"}),": For responsive robot interaction, target < 500ms total speech-to-response. With base model at ~200ms, you have ~300ms for planning and execution."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"audio-pipeline",children:"Audio Capture Pipeline"}),"\n",(0,i.jsx)(n.h3,{id:"ros-2-audio-capture-node",children:"ROS 2 Audio Capture Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""Audio capture node for robot voice commands."""\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nimport numpy as np\r\nimport sounddevice as sd\r\nfrom collections import deque\r\nimport threading\r\n\r\n\r\nclass AudioCaptureNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'audio_capture\')\r\n\r\n        # Parameters\r\n        self.sample_rate = 16000\r\n        self.chunk_duration = 0.1  # 100ms chunks\r\n        self.chunk_samples = int(self.sample_rate * self.chunk_duration)\r\n\r\n        # Buffer for audio\r\n        self.audio_buffer = deque(maxlen=int(10 * self.sample_rate))  # 10s max\r\n\r\n        # VAD state\r\n        self.is_speaking = False\r\n        self.silence_threshold = 0.01\r\n        self.speech_frames = []\r\n\r\n        # Publishers\r\n        self.text_pub = self.create_publisher(String, \'/speech/text\', 10)\r\n\r\n        # Whisper\r\n        from faster_whisper import WhisperModel\r\n        self.whisper = WhisperModel("base", device="cuda")\r\n\r\n        # Start audio stream\r\n        self.stream = sd.InputStream(\r\n            samplerate=self.sample_rate,\r\n            channels=1,\r\n            dtype=np.float32,\r\n            blocksize=self.chunk_samples,\r\n            callback=self.audio_callback\r\n        )\r\n        self.stream.start()\r\n\r\n        self.get_logger().info(\'Audio capture started\')\r\n\r\n    def audio_callback(self, indata, frames, time_info, status):\r\n        """Called for each audio chunk."""\r\n        audio = indata[:, 0]  # Mono\r\n\r\n        # Simple energy-based VAD\r\n        energy = np.sqrt(np.mean(audio ** 2))\r\n\r\n        if energy > self.silence_threshold:\r\n            if not self.is_speaking:\r\n                self.get_logger().info(\'Speech started\')\r\n                self.is_speaking = True\r\n                self.speech_frames = []\r\n\r\n            self.speech_frames.append(audio.copy())\r\n\r\n        elif self.is_speaking:\r\n            # Silence after speech - transcribe\r\n            self.is_speaking = False\r\n            self.get_logger().info(\'Speech ended, transcribing...\')\r\n\r\n            # Concatenate speech frames\r\n            audio_data = np.concatenate(self.speech_frames)\r\n\r\n            # Transcribe in separate thread\r\n            threading.Thread(\r\n                target=self.transcribe_and_publish,\r\n                args=(audio_data,)\r\n            ).start()\r\n\r\n    def transcribe_and_publish(self, audio: np.ndarray):\r\n        """Transcribe audio and publish result."""\r\n        segments, _ = self.whisper.transcribe(audio, language="en")\r\n        text = " ".join([seg.text for seg in segments]).strip()\r\n\r\n        if text:\r\n            msg = String()\r\n            msg.data = text\r\n            self.text_pub.publish(msg)\r\n            self.get_logger().info(f\'Transcribed: "{text}"\')\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = AudioCaptureNode()\r\n    rclpy.spin(node)\r\n    node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"voice-activity-detection-vad",children:"Voice Activity Detection (VAD)"}),"\n",(0,i.jsx)(n.p,{children:"The simple energy-based VAD above works for quiet environments. For noisy robot environments:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# WebRTC VAD for better performance\r\nimport webrtcvad\r\n\r\nclass RobustVAD:\r\n    def __init__(self, aggressiveness: int = 2):\r\n        # 0: least aggressive, 3: most aggressive\r\n        self.vad = webrtcvad.Vad(aggressiveness)\r\n        self.sample_rate = 16000\r\n        self.frame_duration_ms = 30  # 10, 20, or 30 ms\r\n\r\n    def is_speech(self, audio_frame: bytes) -> bool:\r\n        """Check if audio frame contains speech."""\r\n        return self.vad.is_speech(audio_frame, self.sample_rate)\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"conditioning",children:"Prompt Conditioning"}),"\n",(0,i.jsx)(n.h3,{id:"domain-specific-recognition",children:"Domain-Specific Recognition"}),"\n",(0,i.jsx)(n.p,{children:"Whisper can be conditioned on expected vocabulary:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def transcribe_robot_command(self, audio: np.ndarray) -> str:\r\n    """Transcribe with robot command conditioning."""\r\n    segments, _ = self.whisper.transcribe(\r\n        audio,\r\n        language="en",\r\n        task="transcribe",\r\n        initial_prompt=(\r\n            "Robot commands: pick up, put down, move to, "\r\n            "navigate to, stop, go, grab, release, "\r\n            "shelf, table, mug, cup, box, "\r\n            "red, blue, green, left, right"\r\n        )\r\n    )\r\n    return " ".join([seg.text for seg in segments]).strip()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"vocabulary-boosting",children:"Vocabulary Boosting"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Without Conditioning"}),(0,i.jsx)(n.th,{children:"With Conditioning"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:'"Pickup the read mug"'}),(0,i.jsx)(n.td,{children:'"Pick up the red mug"'})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:'"Navigate to shelf too"'}),(0,i.jsx)(n.td,{children:'"Navigate to shelf two"'})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:'"Stop their"'}),(0,i.jsx)(n.td,{children:'"Stop there"'})]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"noise",children:"Noise Robustness"}),"\n",(0,i.jsx)(n.h3,{id:"robot-environmental-noise",children:"Robot Environmental Noise"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Noise Source"}),(0,i.jsx)(n.th,{children:"Frequency Range"}),(0,i.jsx)(n.th,{children:"Impact on ASR"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"DC motors"}),(0,i.jsx)(n.td,{children:"100-500 Hz"}),(0,i.jsx)(n.td,{children:"Moderate"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Servo motors"}),(0,i.jsx)(n.td,{children:"1-5 kHz"}),(0,i.jsx)(n.td,{children:"High"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"HVAC"}),(0,i.jsx)(n.td,{children:"50-200 Hz"}),(0,i.jsx)(n.td,{children:"Low"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Background speech"}),(0,i.jsx)(n.td,{children:"100-3000 Hz"}),(0,i.jsx)(n.td,{children:"High"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"mitigation-strategies",children:"Mitigation Strategies"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Directional Microphone"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Use USB directional mic pointed at user\r\n# Reduces off-axis noise by 10-20 dB\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Noise Gate"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def apply_noise_gate(audio: np.ndarray, threshold: float = 0.02) -> np.ndarray:\r\n    """Zero samples below threshold."""\r\n    return np.where(np.abs(audio) < threshold, 0, audio)\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Spectral Subtraction"})," (for stationary noise):"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import scipy.signal as signal\r\n\r\ndef spectral_subtract(audio: np.ndarray, noise_profile: np.ndarray) -> np.ndarray:\r\n    """Subtract estimated noise spectrum."""\r\n    # Compute spectrograms\r\n    f, t, Zxx = signal.stft(audio)\r\n    f_n, t_n, Zxx_n = signal.stft(noise_profile)\r\n\r\n    # Subtract noise magnitude (keep phase)\r\n    noise_mag = np.mean(np.abs(Zxx_n), axis=1, keepdims=True)\r\n    cleaned_mag = np.maximum(np.abs(Zxx) - noise_mag, 0)\r\n    cleaned = cleaned_mag * np.exp(1j * np.angle(Zxx))\r\n\r\n    # Inverse STFT\r\n    _, cleaned_audio = signal.istft(cleaned)\r\n    return cleaned_audio\n'})}),"\n",(0,i.jsx)(n.h3,{id:"noise-robustness-testing",children:"Noise Robustness Testing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def evaluate_noise_robustness(asr, test_commands, noise_levels):\r\n    """Evaluate ASR accuracy at different noise levels."""\r\n    results = {}\r\n\r\n    for snr_db in noise_levels:\r\n        correct = 0\r\n        for command, audio in test_commands:\r\n            # Add noise at specified SNR\r\n            noisy_audio = add_noise(audio, snr_db)\r\n            transcribed = asr.transcribe(noisy_audio)\r\n\r\n            if transcribed.lower() == command.lower():\r\n                correct += 1\r\n\r\n        accuracy = correct / len(test_commands)\r\n        results[snr_db] = accuracy\r\n        print(f"SNR {snr_db} dB: {accuracy:.1%} accuracy")\r\n\r\n    return results\n'})}),"\n",(0,i.jsx)(n.p,{children:"Expected results:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"SNR"}),(0,i.jsx)(n.th,{children:"Accuracy"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Clean"}),(0,i.jsx)(n.td,{children:"95%+"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"20 dB"}),(0,i.jsx)(n.td,{children:"90%+"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"10 dB"}),(0,i.jsx)(n.td,{children:"75-85%"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"5 dB"}),(0,i.jsx)(n.td,{children:"50-70%"})]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"This chapter covered speech recognition for robot commands:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Whisper architecture"})," converts audio to text via mel spectrograms and transformer encoder-decoder."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Model size selection"})," trades accuracy for latency; ",(0,i.jsx)(n.code,{children:"base"})," is good for real-time robot commands."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Audio capture pipeline"})," with VAD segments speech for transcription."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Prompt conditioning"})," improves accuracy on robot-specific vocabulary."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Noise robustness"})," requires mitigation strategies for real robot environments."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"self-assessment-questions",children:"Self-Assessment Questions"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Model Selection"}),": Your robot needs < 300ms speech-to-text latency. Which Whisper model size would you choose?"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Latency Budget"}),": Speech capture takes 2s (user speaking). Whisper takes 200ms. LLM will take 500ms. Is total latency acceptable for interactive command?"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"VAD Tuning"}),": Your robot's motors create continuous 50 Hz hum. How would you adjust VAD to avoid false speech detection?"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Noise Impact"}),": The robot is in a warehouse with forklifts. SNR is approximately 10 dB. What accuracy can you expect? What would help?"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Prompt Conditioning"}),": Design an initial prompt for a warehouse robot that picks items from shelves. What vocabulary would you include?"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"whats-next",children:"What's Next"}),"\n",(0,i.jsxs)(n.p,{children:["In ",(0,i.jsx)(n.a,{href:"/module-5/chapter-2-llm-task-planning",children:"Chapter 2: LLM Task Planning"}),", you'll design prompts that translate natural language commands into structured robot task plans."]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(a,{...e})}):a(e)}},8453(e,n,r){r.d(n,{R:()=>o,x:()=>d});var s=r(6540);const i={},t=s.createContext(i);function o(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);